---
title: "EDM_all_Spp_all_dams"
author: "Alison Iles"
date: "2/13/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE,
                      warning = FALSE,
                      message = FALSE,
                      tidy.opts=list(width.cutoff=60),
                      tidy=TRUE)
```
# CORE_EDM research questions

Is there evidence of a (strong, consistent) negative impact on salmon returns of pinniped pop increases in the Columbia Basin?
If so, for which runs? Does this effect match local pred occupancy?
Does it depend on the run timing or sub-basin?
How are culls predicted to affect salmon returns and other components of system? 

```{r}
# loading R packages
library(rEDM) # All the EDM analyses are carried out by the rEDM package.
library(Kendall)
library(tidyr)
library(tidyverse)
library(dplyr)
library(psych)
#library(statcomp) #permutation entropy function here - can't get it to install the package. Used my PE function in Matlab
```

# Load and prepare data
```{r}

folder <- "/Users/alisoniles/Google Drive/Coordinated Recoveries/CORE_EDM/Analysis/Coordinated-Recoveries-GitHub/EDM Analysis all Spp all dams/data/"
file_list <- list.files(path=folder, pattern="*.csv") # create list of all .csv files in folder

# read in each .csv file in file_list and create a data frame with the same name as the .csv file
for (i in 1:length(file_list)){
  assign(substr(file_list[i],1,nchar(file_list[i])-4),
  read.csv(paste(folder, file_list[i], sep=''))
  )}

rm(WFA, WAN) #Don't include WFA or WAN as there is not enough years 
rm(TDA, JDA, MCN, RIS, RRH, WEL, LMN, LGS, LGR) #Only keep the first dam for each basin

BON <- select(BON, c("year", "coho", "steelhead", "sockeye", "chinook_SS", "chinook_F")) 
PRD <- select(PRD, c("year", "coho", "steelhead", "sockeye", "chinook_SS", "chinook_F")) 
IHR <- select(IHR, c("year", "coho", "steelhead", "sockeye", "chinook_SS", "chinook_F")) 

# Use gather to take multiple columns and collapse into key-value pairs
BON <- gather(BON, "species", abundance, 2:6)
PRD <- gather(PRD, "species", abundance, 2:6)
IHR <- gather(IHR, "species", abundance, 2:6)

# Add location and source columns:
BON$location <- "BON"
PRD$location <- "PRD"
IHR$location <- "IHR"
BON$source <- "fpc.org"
PRD$source <- "fpc.org"
IHR$source <- "fpc.org"

# Remove string of zeros in early years for coho at PRD
PRD <- PRD %>% slice(16:285)

# Data normalization: Prior to simplex projection, the time series are normalized to zero mean and unit variance.
BON <- BON %>%
  group_by(species) %>%
  mutate(norm = (abundance - mean(abundance)) / sd(abundance)) %>%
  ungroup()
PRD <- PRD %>%
  group_by(species) %>%
  mutate(norm = (abundance - mean(abundance)) / sd(abundance)) %>%
  ungroup()
IHR <- IHR %>%
  group_by(species) %>%
  mutate(norm = (abundance - mean(abundance)) / sd(abundance)) %>%
  ungroup()

# Rearrange
BON <- BON[, c(4,2,1,3,6,5)]
PRD <- PRD[, c(4,2,1,3,6,5)]
IHR <- IHR[, c(4,2,1,3,6,5)]

#Pacific Decadal Oscillation data
PDO <- rename(PDO, abundance = PDOya) 
PDO <- rename(PDO, year = Year) 
PDO$species <- "PDO"
PDO$location <- "ocean"
PDO$source <- "ncdc.noaa.gov"
PDO <- PDO %>%  mutate(norm = (abundance - mean(abundance)) / sd(abundance)) # normalization
PDO <- PDO[, c(4,3,1,2,6,5)]

#Lagged PDO by 1 year
PDO_L1 <- PDO 
PDO_L1$abundance[2:nrow(PDO_L1)] <- PDO_L1$abundance[1:(nrow(PDO_L1)-1)]
PDO_L1$norm[2:nrow(PDO_L1)] <- PDO_L1$norm[1:(nrow(PDO_L1)-1)]
PDO_L1 <- PDO_L1 %>% slice(2:nrow(PDO_L1))
PDO_L1$species <- "PDO_L1"

#Lagged PDO by 2 years
PDO_L2 <- PDO 
PDO_L2$abundance[3:nrow(PDO_L2)] <- PDO_L2$abundance[1:(nrow(PDO_L2)-2)]
PDO_L2$norm[3:nrow(PDO_L2)] <- PDO_L2$norm[1:(nrow(PDO_L2)-2)]
PDO_L2 <- PDO_L2 %>% slice(3:nrow(PDO_L2))
PDO_L2$species <- "PDO_L2"

#Lagged PDO by 3 years
PDO_L3 <- PDO 
PDO_L3$abundance[4:nrow(PDO_L3)] <- PDO_L3$abundance[1:(nrow(PDO_L3)-3)]
PDO_L3$norm[4:nrow(PDO_L3)] <- PDO_L3$norm[1:(nrow(PDO_L3)-3)]
PDO_L3 <- PDO_L3 %>% slice(4:nrow(PDO_L3))
PDO_L3$species <- "PDO_L3"

#Lagged PDO by 4 years
PDO_L4 <- PDO 
PDO_L4$abundance[5:nrow(PDO_L4)] <- PDO_L4$abundance[1:(nrow(PDO_L4)-4)]
PDO_L4$norm[5:nrow(PDO_L4)] <- PDO_L4$norm[1:(nrow(PDO_L4)-4)]
PDO_L4 <- PDO_L4 %>% slice(5:nrow(PDO_L4))
PDO_L4$species <- "PDO_L4"

#Lagged PDO by 5 years
PDO_L5 <- PDO 
PDO_L5$abundance[6:nrow(PDO_L5)] <- PDO_L5$abundance[1:(nrow(PDO_L5)-5)]
PDO_L5$norm[6:nrow(PDO_L5)] <- PDO_L5$norm[1:(nrow(PDO_L5)-5)]
PDO_L5 <- PDO_L5 %>% slice(6:nrow(PDO_L5))
PDO_L5$species <- "PDO_L5"

#Lagged PDO by 6 years
PDO_L6 <- PDO 
PDO_L6$abundance[7:nrow(PDO_L6)] <- PDO_L6$abundance[1:(nrow(PDO_L6)-6)]
PDO_L6$norm[7:nrow(PDO_L6)] <- PDO_L6$norm[1:(nrow(PDO_L6)-6)]
PDO_L6 <- PDO_L6 %>% slice(7:nrow(PDO_L6))
PDO_L6$species <- "PDO_L6"

#Southern Resident Killer Whale data
SRKW <- rename(SRKW, abundance = SR_Orca) 
SRKW <- rename(SRKW, year = Year)
SRKW$species <- "SRKW"
SRKW$location <- "ocean"
SRKW$source <- "www.whaleresearch.com"
SRKW <- SRKW %>%  mutate(norm = (abundance - mean(abundance)) / sd(abundance)) # normalization
SRKW <- SRKW[, c(4,3,1,2,6,5)]

#California Sea Lion data
CSL <- select(CSL, c("Year", "Male"))
CSL <- rename(CSL, year = Year) #rename year column
CSL <- rename(CSL, CSL = Male) 
CSL <- gather(CSL, "species", abundance, 2)
CSL$location <- "ocean"
CSL$source <- "Laake 2018"
CSL <- CSL %>%
  group_by(species) %>%
    mutate(norm = (abundance - mean(abundance)) / sd(abundance)) %>%
      ungroup()
CSL<- CSL[, c(4,2,1,3,6,5)]

dd <- bind_rows(IHR, CSL, SRKW, PDO, PDO_L1, PDO_L2, PDO_L3, PDO_L4, PDO_L5, PDO_L6)
dd$ID <- as.numeric(as.factor(paste0(dd$location, "_", dd$species)))
dd_ts <- dd %>% group_by(ID) %>% nest()
dd_ts$N <- unlist(lapply(dd_ts$data, nrow))

```
# Simplex projection (Sugihara & May 1990)

The complexity of a system can be practically defined as the number of independent variables needed to reconstruct the attractor (i.e. dimensionality of the system). Determining the best embedding dimension E with 'Simplex projection' is a fundamental first step in all EDM analysis. 
By simplex projection, we make one-step, out-of-sample forward forecast (predict t+1 step) using different values of E to determine the optimal embedding dimension. In the case where the time series is rather short, leave-one-out cross-validation can be performed instead of dividing the time series into halves (Sugihara et al. 1996; Glaser et al. 2014).

The general approach was to divide the time series into a training and a test dataset, according to a specified split (e.g. split 2 means first 50% of data used for training and second 50% used for testing). We used a split of 3 (66% of data used for training, 33% used for testing).

We first fitted the simplex projection to identify the best embedding dimension E. E varied from 1 to 10. Based on cross-validation, we chose E with the highest forecast skill across the training data. Next we fitted the S-maps with the chosen E to the training data and determined the optimal tuning parameter theta. Theta determines how non-linear the system is and hence which embeddings are considered for CCM. Theta varied in 18 steps between 0 and 8 (with log scaled step sizes). 

```{r}
# define how much data you want to use
# by division -> 2 = 50% split, 4 = 25% etc.
split <- 2

dd_ts$E <- list(NULL)
dd_ts$theta <- list(NULL)

pdf("/Users/alisoniles/Google Drive/Coordinated Recoveries/CORE_EDM/Analysis/Coordinated-Recoveries-GitHub/EDM Analysis all Spp all dams/output/E_theta_fit_simplex.pdf")

for (i in 1:nrow(dd_ts)){
  
  layout(matrix(c(1,2,3,4), 2, 2, byrow = TRUE))
  ts_select <- as.data.frame(dd_ts$data[[i]])
  step_ahead <- as.integer(nrow(ts_select)/split) # define steps ahead forecast (based on split)
  
  # estimate E_hat, but restrict E to be less than half the ts length to be predicted
  simplex_fit <- simplex(ts_select$norm, 
                     lib = c(1, (length(ts_select$norm)-step_ahead)),  
                     stats_only = T,
                     norm_type = c("L2 norm"), 
                     E = 1:10)
  ## The optimal embedding dimension determined by maximizing rho
  E_hat <- min(simplex_fit[which(max(simplex_fit$rho)==simplex_fit$rho)[1], ]$E, step_ahead/2)
  plot(simplex_fit$E, simplex_fit$rho, type="l", xlab = "Embedding dimension (E)", ylab="Forecast skill (rho)")+
      abline(v=E_hat, col="red", lty=2)
  
  # estimate theta
  smap_fit <- s_map(ts_select$norm, 
                    lib = c(1, (length(ts_select$norm)-step_ahead)),  
                    stats_only = T, 
                    norm_type = c("L2 norm"), 
                    E = E_hat, 
                    theta = c(0, 1e-04, 3e-04, 0.001, 0.003, 0.01, 0.03, 0.1, 0.3, 0.5, 0.75, 1, 1.5, 2, 3, 4, 6, 8, 10))
  theta_hat <- smap_fit[which(max(smap_fit$rho)==smap_fit$rho), ]$theta
  plot(smap_fit$theta, smap_fit$rho, type="l", xlab = "Theta", ylab="Forecast skill (rho)")
      abline(v=theta_hat, col="red", lty=2)
  
  dd_ts$E[i] <- E_hat
  dd_ts$theta[i] <- theta_hat
  
  # Evaluate tau
   smap_tau  <- s_map(ts_select$norm, 
                      lib = c(1, (length(ts_select$norm)-step_ahead)), 
                      pred = c((length(ts_select$norm)-step_ahead)+1, length(ts_select$norm)), 
                      stats_only = F, 
                      norm_type = c("L2 norm"), 
                      E = E_hat, 
                      theta = theta_hat,
                      tau = 1:6)
   tau_hat <- smap_tau[which(max(smap_tau$rho)==smap_tau$rho), ]$tau
   plot(smap_tau$tau, smap_tau$rho, type="l", xlab = "tau", ylab="Forecast skill (rho)")
      abline(v=tau_hat, col="red", lty=2)
   
  # best fit 
  smap_pred  <- s_map(ts_select$norm, 
                      lib = c(1, (length(ts_select$norm)-step_ahead)), 
                      pred = c((length(ts_select$norm)-step_ahead)+1, length(ts_select$norm)), 
                      stats_only = F, 
                      norm_type = c("L2 norm"), 
                      E = E_hat, 
                      theta = theta_hat)
  
  xyy <- smap_pred["model_output"][[1]][[1]] %>% drop_na()
  plot(ts_select$year, ts_select$norm, 
       type="l", 
       xlab = "Time", 
       ylab="Abundance (normalized)", 
       main=paste(ts_select$location[[1]], ts_select$species[[1]], sep=" "))
  lines(ts_select$year[xyy$time[1]:xyy$time[nrow(xyy)]], xyy$pred, col=2)
  
  layout(matrix(c(1,1), 1, 1, byrow = TRUE))
}

dev.off()
```
# Determining causal variables by convergent cross mapping (CCM)

EDM can be used to reveal causation between variables. Two variables are causally linked if they interact in the same dynamical system. Following Takens’ theorem, the system manifold reconstructed from univariate embedding (State Space Reconstruction using a single variable) gives a 1-1 map to the original system, i.e., topologically invariance. Because all manifolds reconstructed from univariates give 1-1 maps to the original manifold, it is not surprising that all the reconstructed manifolds result in 1-1 mappings if they are causally linked. Based on this idea, Sugihara et al. (2012) developed a cross-mapping algorithm to test the causation between a pair of variables in dynamical systems. This algorithm predicts the current quantity of one variable M1 using the time lags of another variable M2 and vice versa. If M1 and M2 belong to the same dynamical system (i.e., they are causally linked), the cross-mapping between them shall be ‘‘convergent.’’ Convergence means that the cross-mapping skill (q) improves with increasing library size. This is because more data in the library makes the reconstructed manifold denser, and the highly resolved attractor improves the accuracy of prediction based on neighboring points (i.e., simplex projection). Sugihara et al. (2012) stated that convergence is a practical criterion to test causation, and called this phenomenon convergent cross-mapping (CCM).

To evaluate convergence in cross-mapping, the state space is reconstructed using different library lengths (L) subsampled randomly from time series. Here, Li starts from the minimal library length, L0, which is equal to the embedding dimension, to the maximal library length, Lmax, which equal to the whole length of the time series. To test the convergence of CCM, two approaches are widely used. First, the convergence can be tested by investigating how the cross- mapping skill changes with respect to the library size (e.g., trend or increment). For example, one can consider the following two statistical criteria: (1) testing the existence of a significant monotonic increasing trend in q(L) using Kendall’s s test, and (2) testing the significance of the improvement in q(L) by Fisher’s Dq Z test, which checks whether the cross-mapping skill obtained under the maximal library length (q(Lmax)) is significantly higher than that obtained using the minimal library length (q(L0)). The convergence of CCM is deemed significant when both Kendall’s s test and Fisher’s Dq Z test are significant. 

Note that, the direction of cross-mapping is opposite to the direction of cause-effect. That is, a convergent cross-mapping from M2(t) to M1(t) indicates that M1 causes M2. This is because M1, as a causal variable driving M2, has left its footprints on M2(t). The footprints of M1 are transcribed on the past history of M2, and thus M2 is able to predict the current value of M1.

# Test the causal significance between each variable

The matrix output of the calculations will be organized with the convention of the community matrix, A, whose elements aij are interpreted as the direct effect of the species in column j on the species in row i. 

```{R}
pdf("/Users/alisoniles/Google Drive/Coordinated Recoveries/CORE_EDM/Analysis/Coordinated-Recoveries-GitHub/EDM Analysis all Spp all dams/output/ccm_IHR_Fig.pdf")

#output matrices of interaction rho and causal significance tests to determine which to use in the embeddings. 
SI_embed <- matrix(ncol=nrow(dd_ts), nrow=nrow(dd_ts)) #TRUE or FALSE from all significance tests
SI_rho <- matrix(ncol=nrow(dd_ts), nrow=nrow(dd_ts)) # Rho from CCM of predicted vs actual 
    var_names <- matrix(1, nrow=nrow(dd_ts)) #To store variable names and use as column and row headings 
    colnames(SI_embed, do.NULL = FALSE)
    colnames(SI_rho, do.NULL = FALSE)
    rownames(SI_embed, do.NULL = FALSE)
    rownames(SI_rho, do.NULL = FALSE)

# If you can predict j based on the library formed by i, then the signiture of j is found in the time series of i and thus j has a direct, causal influence on i
for (j in 1:nrow(dd_ts)){   # the direct effect of the variable in column j
    for (i in 1:nrow(dd_ts)){   # on the variable in row i
        if (i==j) next
        layout(matrix(c(1,1), 1, 1, byrow = TRUE))

        tar_y <- as.data.frame(dd_ts$data[[j]]) #select target variable data 
        lib_x <- as.data.frame(dd_ts$data[[i]]) #select variable for which to base the library on
        
        # select overlapping years
        dat <- merge(lib_x,tar_y, by = "year")
        dat <- select(dat, c("year", "abundance.x", "abundance.y")) 
        dat[,2:3] <- as.data.frame(apply(dat[,2:3], 2, function(x) (x-mean(x))/sd(x)))
        dat <- rename(dat, tar.y = abundance.y) 
        dat <- rename(dat, lib.x = abundance.x)
        ls <- nrow(dat)
        
        # Determine the embedding dimension
        E.test=NULL
        for(E.t in 1:10){
            E.temp <- ccm(dat, 
                    E = E.t, 
                    lib_column = "lib.x", 
                    target_column = "tar.y", 
                    lib_sizes = ls, 
                    num_samples = 1, 
                    tp=-1,
                    random_libs = F)
            E.test=rbind(E.test,E.temp)}
        E <- E.test$E[which.max(E.test$rho)[1]] # the optimal E
        
        # Design a sequence of library size
        libs <- c(seq(as.integer(E*2),ls,1))
        
        # CCM analysis with varying library size (L)
        xmap <- ccm(dat,
                    E=E,
                    lib_column="lib.x", 
                    target_column="tar.y",
                    lib_sizes=libs, 
                    num_samples=100, 
                    replace=T, 
                    RNGseed=2301)
        
        # Calculate the median, maximum, and 1st & 3rd quantile of rho
        rho_quant=as.matrix(aggregate(xmap[,c('rho')],by = list(as.factor(xmap$lib_size)), quantile, na.rm = TRUE)[,'x'])
        rho_quant <- rho_quant[complete.cases(rho_quant*0),1:5] #removes rows with inf values
        
        #Test for monotonic trend in a time series z[t] based on the Kendall rank correlation of z[t] and t. 
        #Here the median, maximum, and 1st & 3rd quantiles are tested, all need to be <0.05.
        MK <- apply(rho_quant[,2:5],2,MannKendall) 
        
        # Transforms the rho at the min library and max library length to a normally distributed Fisher's z value
        # Tests that the max library z is significantly higher than the begining. 
        # Independent correlations, different sample sizes. Our small sample sizes are a disadvantage here.
        FZ <- paired.r(rho_quant[1,3],rho_quant[dim(rho_quant)[1],3], NULL, libs[1], libs[dim(rho_quant)[1]], twotailed=FALSE) 
        
        # Test if rho is significantly different from 0 at p=0.05 level (done for Sockey in Ye et al. 2015)
        CI95 <- r.con(rho_quant[dim(rho_quant)[1],3], libs[dim(rho_quant)[1]], p=.95, twotailed=FALSE) 
        
        # Output tables for the embeddings
        #Criteria for inclusion: all percentiles are increasing and the final rho is statistically greater than zero. 
        SI_embed[i,j] <- (MK$`25%`[[2]]<0.05 & 
                          MK$`50%`[[2]]<0.05 & 
                          MK$`75%`[[2]]<0.05 & 
                          MK$`100%`[[2]]<0.05 & 
                          CI95[1] > 0 & 
                          (rho_quant[1,3] < rho_quant[dim(rho_quant)[1],3])) 
        SI_rho[i,j] <- signif(rho_quant[dim(rho_quant)[1],3],2)
        var_names[j] <- dd_ts$data[[j]][[2]][[1]]
        
        # median predictive skill vs library size (or we can use mean predictive skill)
        plot(rho_quant[,3]~libs[1:dim(rho_quant)[1]],
             type="l",
             col="red",
             ylim=c(-0.1,1.1),
             lwd=2,
             main=paste(dd_ts$data[[i]][[1]][[1]], dd_ts$data[[i]][[2]][[1]], "xmap",  dd_ts$data[[j]][[1]][[1]], dd_ts$data[[j]][[2]][[1]], sep=" "), 
             sub=paste("(i.e. testing", dd_ts$data[[j]][[1]][[1]], dd_ts$data[[j]][[2]][[1]],"as a cause of",dd_ts$data[[i]][[1]][[1]], dd_ts$data[[i]][[2]][[1]], ")", sep=" "),
             xlab="Library size",
             ylab=expression(rho)) # median predictive skill vs library size (or we can use mean predictive skill)
        lines(rho_quant[,2]~libs[1:dim(rho_quant)[1]],col="red",lwd=1,lty=2) # 1st quantile 
        lines(rho_quant[,4]~libs[1:dim(rho_quant)[1]],col="red",lwd=1,lty=2) # 3rd quantile
        
        text(30,1.05,paste("MK @ each quantile < 0.05?:", SI_embed[i,j], sep=" "))
        text(30,1,paste("begin-end FZ p-val:", signif(FZ$p,2), sep=" "))
        text(30,0.95,paste("upper 95% CI:", signif(CI95[2],2), sep=" "))
        text(30,0.9,paste("lower 95% CI:", signif(CI95[1],2), sep=" "))
        
        layout(matrix(c(1,1), 1, 1, byrow = TRUE))
  }
}
dev.off()

SI_embed <- data.frame(SI_embed)
SI_rho <- data.frame(SI_rho)
colnames(SI_embed) <- rownames(SI_embed) <- colnames(SI_rho) <- rownames(SI_rho) <- var_names


```
# Tracking interactions in time (Code from Deyle 2016)

To calculate the coefficients of the partial effect of one variable on another, we first have to determine which embedding to use. This is somewhat subjective. Although only 'causal' variables should be included, our limited sample size makes the more rigorous statistical tests of causality (significantly different rho between min and max library size in the CCM analysis) 

```{r}
library(Matrix) 
library(quantreg)

pdf("/Users/alisoniles/Google Drive/Coordinated Recoveries/CORE_EDM/Analysis/Coordinated-Recoveries-GitHub/EDM Analysis all Spp all dams/output/Coeff_Fig.pdf")

for (i in 1:7){ #for each of the five fish taxa + CSL + SRKW
  
  layout(matrix(c(1,2,3), 3, 1, byrow = TRUE))
 
  PDO_L <- which(max(SI_rho[i,8:14])==SI_rho[i,8:14])+7
  plot(seq(from = 0, to = 6, by = 1), SI_rho[i,8:14], type="l", xlab = "PDO prediction lag (yrs)", ylab="Forecast skill (rho)")
  points(which(max(SI_rho[i,8:14])==SI_rho[i,8:14])-1, max(SI_rho[i,8:14]), type="p", col="red3")
 
  which_var <- c(i, which(SI_embed[i,1:7]==TRUE), PDO_L) #Identifiy the variables to include in the analysis
  Edim <- length(which_var)
  
  Embedding <- dd_ts$data[[i]][[2]][[1]]
  d <- as.data.frame(dd_ts$data[[i]]) #select target species data 
  dname <- d$species[[1]]
  d <- select(d, c("year", "abundance")) 
  names(d) <- c("year", dname)
    for (wv in 2:length(which_var)){
    Embedding <- c(Embedding, dd_ts$data[[which_var[wv]]][[2]][[1]])
    dvar <- dd_ts$data[[which_var[wv]]]
    dname <- dvar$species[[1]] 
    dvar <- select(dvar, c("year", "abundance")) 
    names(dvar) <- c("year", dname)
    d <- merge(d, dvar, by = "year") #merge the different variables together by overlapping years
    }
  year <- as.data.frame(d$year) %>% slice(2:nrow(d))
  d <- select(d, Embedding) 
  ls <- nrow(d)
 
  targ_col <- 1
   
  #For the weighted linear regression, we will be
  block <- cbind(d[2:dim(d)[1],targ_col],d[1:(dim(d)[1]-1),]) 
  norm_consts <- apply(block, 2, function(x) sd(x))
  block <- as.data.frame(apply(block, 2, function(x) (x-mean(x))/sd(x)))
  names(block) <- c("1ahead",Embedding)
   
  #We have a few final parameters to set for the S-map regression.
  lib <- 1:dim(block)[1] 
  pred <- 1:dim(block)[1] 
   
   # Explore the best weighting parameter (nonlinear parameter, theta)
               test_theta <- block_lnlp(block, method = "s-map", num_neighbors = 0,
               theta = c(0, 1e-04, 3e-04, 0.001,0.003, 0.01, 0.03, 0.1,0.3, 0.5, 0.75, 1, 1.25, 1.5, 1.75, 2), #try many thetas
               target_column = targ_col,
               silent = T)
               best_theta <- test_theta[which.min(test_theta$mae),"theta"]  # Best theta is selected based on mean absolute error (MAE)
                    # Check MAE and theta
                    plot(test_theta$mae~test_theta$theta, type="l", xlab="Theta", ylab="MAE")+
                    points(best_theta, min(test_theta$mae), type="p", col="red3")

               # Do S-map analysis with the best theta
               smap_res <-  block_lnlp(block,
               method = "s-map",
               num_neighbors = 0, # we have to use any value < 1 for s-map
               theta = best_theta,
               target_column = targ_col,
               silent = T,
               save_smap_coefficients = T) # save S-map coefficients
               
               # Time series of fluctuating interaction strength for spp i
               smap_coef <- as.data.frame(smap_res$smap_coefficients[[1]])
               coeff <- data.frame(year, smap_coef)
               coeff_names <- sapply(Embedding, function(x) paste("d", Embedding[targ_col], "/d", x, sep = ""))
               colnames(coeff) <- c("year",coeff_names,"Intercept")
               coeff <- cbind(pred,coeff) 
               colnames(coeff)[1] <- "t"
  
  #We can plot a time series of these coeffcients over a span of time.
  l<-dim(block[1])[1]-1  #number of datapoints for plotting
  trange <- 1:l

  plotcol <- NULL
  # First create an empty plot.
    plot(0,0, type = 'n', xlim = c(min(year), max(year)), 
         ylim = c(min(coeff[trange,3:Edim+1]), max(coeff[trange,2:Edim+1])),
         xlab="time", 
         ylab="interaction coefficient")
    
    # Create a list of colors to use for the lines.
    cl <- rainbow(Edim)
    
    # Now fill plot with the log transformed coverage data from the
    # files one by one.
    for  (k in 1:Edim) {
         lines(coeff[trange,2],coeff[trange,(2+k)], col = cl[k], type="l", lwd=2)
         plotcol[k] <- cl[k]
      }
    abline(a=0 ,b=0 , lty="dashed", col="black", lwd=.5)
    legend("bottomleft", legend = coeff_names, col = cl[], lwd = 1,
           cex = 0.5)
    
  
  layout(matrix(c(1,1), 1, 1, byrow = TRUE))
}
dev.off()
```
 #Finally, to stay consistent with prior applications of S-maps, we will use SVD to solve the weighted linear regression. For ease, we put this into a single function that is similar to the basic lm() function in R. The lm() function could be used instead, but it computes the regression using QR instead of SVD.
  
  lm_svdsolve <- function(y, x, ws, subset = seq_along(y)){ 
        x <- x[subset,]
        y <- y[subset]
        ws <- ws[subset]
      
        # prepended column of 1s for constant term in linear model
        A <- cbind(1, x) * ws 
        A_svd <- svd(A)
        
        # >>REMOVE SMALL SINGULAR VALUES<<
        s <- A_svd$d
        s_inv <- matrix(0, nrow = dim(x)[2]+1, ncol = dim(x)[2]+1) 
        for(k in seq_along(s)){
              if(s[k] >= max(s) * 1e-5) s_inv[k,k] <- 1/s[k]
            }
      
        coeff <- A_svd$v %*% s_inv %*% t(A_svd$u) %*% (ws * y)
        coeff <- t(coeff)
        
        colnames(coeff) <- c("const",colnames(x))
        return(coeff) 
    }

#Now, all the preceding code was just to set up the calculation. We now loop over each prediction point to actually calculate the S-map locally weighted regression for each prediction point. This calculation is quite simple. (1) We calculate the (normalized) Euclidian distance from the target point to the other points on the attractor. (2) These distances then become the weights for a weighted linear regression, which we solve using singular value decomposition (SVD).
  
  # >>CALCULATE COEFFICIENTS<<
  for (ipred in 1:length(pred)){
  
      #target point is excluded from the fitting procedure
      libs = lib[-pred[ipred]]
      
      # >>CALCULATE WEIGHTS<<
      q <- matrix(as.numeric(block[pred[ipred],2:dim(block)[2]]), 
                  ncol=Edim, nrow=length(libs), byrow = T)
      distances <- sqrt(rowSums((block[libs,2:dim(block)[2]] - q)^2)) 
      dbar <- mean(distances)
      Ws <- exp(-theta*distances/dbar)
      
      # >>REGRESS<<
      svd_fit <- lm_svdsolve(block[libs,1],block[libs,2:dim(block)[2]],Ws) 
      coeff[ipred,] <- svd_fit[-1]
    }
  