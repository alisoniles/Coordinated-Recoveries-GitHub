---
title: "3_CCM_pairwise_causality"
author: "Alison Iles"
date: "7/2/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
#load the necessary packages. 
Note that the `echo = FALSE` parameter prevents printing of the R code.

```{r, echo=FALSE}
library(rEDM)
library(reshape2)
library(ggplot2)
library(viridis)
library(gridExtra)
library(xtable)
library(rlist)
library(Kendall) #for MannKendall test
library(psych) #for paired.r function
library(tidyr)
library(stringr)
```

#Convergent Cross Mapping (CCM) with time delays
To test whether pairwise causality exists.. Also to test all the lags for each environmental variable for biological interpretation and to determing the direction of causality. 

```{R}

rm(list=ls())  
load("Data/Rdata/block_data_no_lags.Rdata")  
data <- block_data_no_lags
rownames(data)
      aa <- t(data.frame(as.list(names(block_data_no_lags[[1]]))))
      rownames(aa) <- NULL #remove rownames
      bb <- data.frame(c(aa[40:44], aa[57:62], aa[75:104], aa[106:109], aa[111:113], aa[115:NROW(aa)] ))
      colnames(bb) <- c("variable")
      cc <- str_split_fixed(bb$variable, "[.]",n=2)
var_names <- cbind(bb,cc)
var_names[4] <- matrix(NA, nrow = NROW(var_names), ncol = 1)
      colnames(var_names) <- c("target","cat","subcat","target var E")
      rm(block_data_no_lags, aa, bb, cc)


     #concatenate the time series of each stock together
block <- rbind(data$'Bear Valley Creek', data$'Big Creek', data$'Camas Creek', data$'Catherine Creek', data$'Chamberlain Creek', data$'East Fork Salmon River', data$'East Fork South Fork', data$'Grande Ronde Upper Mainstem', data$'Imnaha River', data$'Lemhi River', data$'Loon Creek', data$'Marsh Creek', data$'Middle Fork Salmon River above Indian Creek', data$'Middle Fork Salmon River below Indian Creek', data$'Minam River', data$'North Fork Salmon River', data$'Pahsimeroi River', data$'Salmon River Lower Mainstem below Redfish Lake', data$'Salmon River Upper Mainstem above Redfish Lake', data$'Secesh River', data$'South Fork Salmon River Mainstem', data$'Sulphur Creek', data$'Tucannon River', data$'Valley Creek', data$'Wallowa River, Hurricane Creek, Bear Creek, and Lostine Rivers', data$'Wenaha River', data$'Yankee Fork')
rownames(block) <- NULL
    library_vars <- c("salm.rec4n", "salm.rec5n")
    
    #For CCM we need to use the best univariate E for each target variable
    lib <- matrix(NA, nrow = length(which(diff(block$year)!=1))+1, ncol = 2)
          lib[,1] <- c(1, which(diff(block$year)!=1)+1)
          lib[,2] <- c(which(diff(block$year)!=1), nrow(block))
          
    D <- as.matrix(block[c(var_names$target)])
    for(i in (1:ncol(D))){
          Simp_out <- simplex(D[479:548,i], E = 1:10, tau=1, silent = TRUE)
          var_names$`target var E`[i] <- min(Simp_out$E[Simp_out$rho>max(Simp_out$rho)-(sd(Simp_out$rho)/2)]) #choose the smallest E whose rho is within 0.5 stdv of the maximum rho
          }
     
    #load optimal theta for library vars
    Etheta <- readRDS(file="Output/Rdata/2_Nonlinearity/best_E_theta_rho_recspn_ESU.RDS")
    
    #set up output table
    env_vars <- as.character(var_names[,1])
    # generate all combinations of lib_column, target_column, tp
    CCM_outputlist <- expand.grid(library = library_vars, 
                      target = env_vars, 
                      tp = -8:8, 
                      stringsAsFactors = FALSE)
    CCM_outputlist <- merge(CCM_outputlist, var_names)
    CCM_outputlist <- merge(CCM_outputlist, Etheta[,1:2])
    CCM_outputlist[,8:14] <- matrix(NA, nrow = NROW(CCM_outputlist), ncol = 7)
    colnames(CCM_outputlist) <- c("library", "target", "tp", "cat", "subcat", "target_E", "library_E", "ccm_rho", "MannKendall", "FisherZ", "L_xmap_T_rho", "L_xmap_T_rho_sd", "T_xmap_L_rho", "T_xmap_L_rho_sd")

    

#---------------------------------------------------------------------------------------------------------  
# Pairwise univariate CCM from library var to target var f  
    i <- 1
     for(i in c(1:NROW(CCM_outputlist)) ){  
    
      library_var <- CCM_outputlist$library[i]
      target_var <- CCM_outputlist$target[i]
      E_lib <- as.numeric(CCM_outputlist$library_E[i])
      E_tar <- as.numeric(CCM_outputlist$target_E[i])
      tp <- as.numeric(CCM_outputlist$tp[i])
      
      ccm_block <- ccm_block <- as.matrix(cbind(block$year, block[library_var], block[target_var]))
            rownames(ccm_block) <- NULL #remove rownames to supress error in CCM: NAs introduced by coersion
            ccm_block <- ccm_block[complete.cases(ccm_block), ]#remove rows with NA
            CCMlibrarynames <- colnames(ccm_block)
            CCMlibrarynames <- CCMlibrarynames[-c(1)] #remove "time" from list of library column names with which to do the CCM
            colnames(ccm_block) <- c("year", CCMlibrarynames)
            

          #List in lib the begin and end points of each stock's data
    lib <- matrix(NA, nrow = length(which(diff(ccm_block[,1])!=1))+1, ncol = 2)
    lib[,1] <- c(1, which(diff(ccm_block[,1])!=1)+1)
    lib[,2] <- c(which(diff(ccm_block[,1])!=1), nrow(ccm_block))
    #only include in the library the sections of data that are continuous for at least 20 time points. 
         minlib <- lib[,2]-lib[,1]
         lib <- lib[minlib>11,] 
         x <- ccm_block[lib[1,1]:lib[1,2],] #narrow the data block to these library sections
             for (r in 2:nrow(lib)){
                 xtmp <- ccm_block[lib[r,1]:lib[r,2],]
                 x <- rbind(x,xtmp)}
         ccm_block <- x
      # need to recreate the library list
    lib <- matrix(NA, nrow = length(which(diff(ccm_block[,1])!=1))+1, ncol = 2)
    lib[,1] <- c(1, which(diff(ccm_block[,1])!=1)+1)
    lib[,2] <- c(which(diff(ccm_block[,1])!=1), nrow(ccm_block))
              rm(x,minlib, xtmp, r)
    
    libsize <- round(seq(NROW(ccm_block)/8, NROW(ccm_block), by = NROW(ccm_block)/8))
    libbootstrapsize <- lib[8,2]
   
ccm <-  ccm(ccm_block, lib = lib, E = E_tar, lib_column = library_var, target_column = target_var, first_column_time = TRUE, tp = tp, lib_sizes = libsize, random_libs = TRUE, num_samples=100, replace=TRUE, RNGseed=2301, silent = TRUE)      

      # Test if rho is significantly different from 0 at p=0.05 level (done for Sockey in Ye et al. 2015)
      # Calculate the median, maximum, and 1st & 3rd quantile of rho
            rho_quant=as.matrix(aggregate(ccm[,c('rho')],by = list(as.factor(ccm$lib_size)), quantile, na.rm = TRUE)[,'x'])
            rho_quant <- rho_quant[complete.cases(rho_quant*0),1:5] #removes rows with inf values
      CCM_outputlist$ccm_rho[i] <- signif(rho_quant[dim(rho_quant)[1],3],2) #save the cross map skill at max library size (50th percentile)
            
      #Test for monotonic trend in a time series z[t] based on the Kendall rank correlation of z[t] and t. 
            #Here the median, maximum, and 1st & 3rd quantiles of rho are tested, all need to be <0.05.
            ccmq=as.matrix(aggregate(ccm[,c('rho')],by = list(as.factor(ccm$lib_size)), quantile)[,'x'])
            ccmMK <- apply(ccmq[,2:5],2,MannKendall)
      CCM_outputlist$MannKendall[i] <- (ccmMK$`25%`[[2]]<0.05 & ccmMK$`50%`[[2]]<0.05 & ccmMK$`75%`[[2]]<0.05) # & ccmMK$`100%`[[2]]<0.05) not the last quantile as the plot often flattens out       
      # Tests that the max library z is significantly higher than the begining. 
      # Transforms the rho at the min library and max library length to a normally distributed Fisher's z value
            # Independent correlations, different sample sizes. Our small sample sizes are a disadvantage here.
      FZ <- paired.r(rho_quant[1,3],rho_quant[dim(rho_quant)[1],3], NULL, libsize[1], libsize[dim(rho_quant)[1]], twotailed=FALSE)
            CCM_outputlist$FisherZ[i] <- FZ$p            
            
     #perform ccm with no random sampling at max library size to save the rho for that tp
        #using lags of library var to crossmap the target var as the putative causal variable
        ccm_tp <-  ccm(ccm_block, 
             lib = lib, pred = lib, 
             E = E_tar, tp = tp, 
             lib_sizes = libbootstrapsize,
             random_libs = TRUE, num_samples = 100, replace = TRUE, 
             lib_column = library_var, 
             target_column = target_var, 
             first_column_time = TRUE, 
             silent = TRUE)
        CCM_outputlist$L_xmap_T_rho[i] <- mean(ccm_tp$rho)
        CCM_outputlist$L_xmap_T_rho_sd[i] <- sd(ccm_tp$rho)
        
        #using lags of target var to crossmap the library var as the putative causal variable
        ccm_tp <-  ccm(ccm_block, 
             lib = lib, pred = lib, 
             E = E_lib, tp = tp, 
             lib_sizes = libbootstrapsize, #library size is equivalent to the joint length of 8 libraries
             random_libs = TRUE, num_samples = 100, replace = TRUE, #100 random libraries sampled with replacement
             lib_column = target_var, 
             target_column = library_var, 
             first_column_time = TRUE, 
             silent = TRUE)
        CCM_outputlist$T_xmap_L_rho[i] <- mean(ccm_tp$rho)
        CCM_outputlist$T_xmap_L_rho_sd[i] <- sd(ccm_tp$rho)
            
#-----------------------------------------------------------------------------------------------------  
     }
    
saveRDS(CCM_outputlist, file = paste("Output/Rdata/CCM_tp_E_tar.RDS", sep = ""), compress = FALSE)
```    

#CCM output list meta data:
    "library": In CCM analysis the 'library' variable is the variable of interest, in our case SRKW population data and birth anomalies
    "target": In CCM analysis the 'target' variable is the putative causal variable, in our case the various abiotic and biotic variables that are hypothesized to affect SRKW
    "tp": the time to prediction; how many time steps ahead (or behind if tp is negative) to forecast the target variable. 
    "cat": The broader category that the target variable is in
    "subcat": The specific variable in the broader target variable categories
    "offset": The number of years that the target variable is offset. For this analysis, it is always zero. Delete this variable.
    "library E": The optimal univariate embedding dimension of the library variable
    "target E": The optimal univariate embedding dimension of the target variable

The analysis uses the ccm function to test the univariate causal influence of the target variable on the library variable. The analysis tests many different library sizes sampled randomly many times to get distributions of rho values (the correlation coefficient between observations and predictions).
    "ccm_rho": The cross map skill at the maximum library size. 
    "MannKendall": A test for convergence, or an increasing monotonic trend in rho as library size increases based on the Kendall rank correlation. Here the first, median and & 3rd quantiles of rho are tested, not the last quantile as the plot often flattens out; all need to be <0.05 for this to be 'TRUE'.  
    "FisherZ": A test that rho at the max library size is significantly higher than at the minimum. 
    "L_xmap_T_rho": The cross map skill at the maximum library size without random sampling
    "T_xmap_L_rho": The cross map skill in the reverse direction: using lags of the target variable to forecast the library variable
    

```{R}
#Figures
rm(list=ls())  
#load("Data/Rdata/block_data_no_lags.Rdata") 
 
 
CCM_outputlist<- readRDS("Output/Rdata/3_CCM/CCM_tp_E_tar.RDS")

    # sort output table by offset then category
    d <-  CCM_outputlist[order(CCM_outputlist$library),]
    d <- d[order(d$tp),]
    d <-  d[order(d$cat),]
    d <-  d[order(d$subcat),]
    d <- d[order(d$library),]
    d$tp <- as.numeric(d$tp)
    
library_vars <- c("salm.rec4n", "salm.rec5n")
    d <- d[d$library=="salm.rec5n",]

manual_color_codes <- read.csv("Data/csv_data/CORE_CCM_figure_color_codes.csv") #csv file containing the manual color codes and labels for plots 
     manual_color_codes <- manual_color_codes[order(manual_color_codes$labels),]
     manual_color_codes <- manual_color_codes[order(manual_color_codes$cat),]
     
varcat <- unique(d$cat) 
for(c in c(1:length(varcat))) {

  d_plot <- d[d$cat==varcat[c],]
  mcc <- manual_color_codes[manual_color_codes$cat==varcat[c],2:4]
  d_plot <- merge(d_plot, mcc, by.x ="subcat", by.y ="breaks")
  d_plot <- d_plot[order(d_plot$labels),]

  if (NROW(mcc) >1) {mcc <- sapply(mcc, unlist)}
  rownames(mcc) <- mcc[,2]
  
  #Plot cross-map skill at different times to prediction steps. Because cross-map skill less than zero is non-informative, we don't show negative values when plotting
  p1 <- ggplot(d_plot)+
    geom_line(aes(x=tp, y=T_xmap_L_rho)) +
          geom_errorbar(aes(x=tp, y=T_xmap_L_rho, ymin=T_xmap_L_rho-T_xmap_L_rho_sd, ymax=T_xmap_L_rho+T_xmap_L_rho_sd), width=0.5) +
    geom_line(aes(x=tp, y=L_xmap_T_rho, col=subcat)) +
          geom_point(aes(x=tp, y=L_xmap_T_rho, col=subcat, shape=(MannKendall==TRUE)), show.legend = TRUE) +
          geom_errorbar(aes(x=tp, y=L_xmap_T_rho, ymin=L_xmap_T_rho-L_xmap_T_rho_sd, ymax=L_xmap_T_rho+L_xmap_T_rho_sd), width=0.5, color=d_plot$values) +
    geom_vline(aes(xintercept=0), colour='#999999') +
    geom_vline(aes(xintercept=5), colour='#999999') +
    geom_hline(aes(yintercept=0), colour='#999999') +
    theme_bw() + 
    guides(col = guide_legend(ncol=1)) +
    labs(title= varcat[c], subtitle = paste("Using lags of ", d[1,1], " to xmap", varcat[c]), x="Time to prediction, years", y=expression(paste("Cross-mapping correlation, ", {rho}))) + 
    scale_color_manual(values = mcc[,1], name="SRKW xmap ...", breaks = mcc[,2], labels = mcc[,3]) +
    scale_shape_manual(name="MK test",  values=c("FALSE"=1, "TRUE"=16),  breaks = c("TRUE", "FALSE"), labels = c("True", "False")) +
    facet_wrap(vars(labels), ncol = 3 ) +
       theme(strip.background = element_blank(), strip.placement = "outside")
print(p1)
ggsave(filename = paste("Output/Figures/3_CCM/CCM_tp_", d[1,1], "_xmap_", varcat[c],".pdf", sep = ""), plot = p1, width = 7, height = 9.5, units = "in") 

}

```

#Intro to Time-delay convergent cross-mapping
Convergent cross mapping (CCM) is a test of causality on observational time series of two variables measured from the same (at least somewhat) deterministic system. Takens’ Theorem (Takens 1981) demonstrates that if x does influence y, then the historical values of x can be recovered from variable y alone. In practical terms, this is accomplished using the technique of “cross mapping”: a time delay embedding is constructed from the time series of y, and the ability to estimate the values of x from this embedding quantifies how much information about x has been encoded into y (Sugihara et al. 2012).  Thus, the causal effect of x on y is determined by how well y cross maps x. However, strong unidirectional forcing from the driving variable can cause “generalized synchrony” where the dynamics of the full system collapses to just that of the driving variable and causality is observed in both directions (i.e., x cross maps y and y cross maps x) even when there is no true causal effect of y on x. Thus, CCM appears to be limited by the fact that it may not be able to distinguish between bidirectional causality and strong unidirectional causality that leads to synchrony.

To distinguish generalize synchrony from bidirectional causality, Ye et al. 2015 demonstrate an extension to CCM with lagged predictions, which is  useful when driving variables act with some time delay on response variables. When "generalized synchrony" is suspected, a negative lag for cross mapping in the true causal direction should be apparent because the response variable is better at predicting the past values of the driving variable rather than future values.  Conversely, the optimal cross map lag in the other direction (when using x to xmap y) is positive - the driving variable best predicts the future of the response variable. This is because even with synchrony, there is no flow of causal information from y to x, and so changes in x are not reflected in y until sometime in the future. Thus, the positive lag from x cross mapping y informs us that there is unidirectional causality, even when the interaction is strong enough to result in synchrony. In the case of true bidirectional causality, optimal cross mapping lags would be negative in both directions. Thus, a time delay in the response of y to x can be used to distinguish between bidirectional causality and generalized synchrony.



The length of a time delay may also have a biological explanation. Ye et al. (2015) analyzed a predator-prey experiment that showed bidirectional causality. While the effect of predators on prey was rapid at -1 lag, the effect of prey on predators showed a distinctly longer lag, as prey ingestion takes time to translate into population growth of the predator. In addition, the identification of time delays in causation can inform our expectation of delays in management interventions as the magnitude of the lag roughly equals the time delay of causality. 

Time delays can also help determine the order of variables in a transitive causal chain by distinguishing direct from indirect effects using optimal cross-map lags and optimal cross-map skill. Ye et al. 2015 demonstrate that optimal cross-mapping has smaller lags and greater skill for more direct effects than indirect effects, although the effects of cross-map skill showed greater variance and were less reliable indicator of direct vs. indirect causation than cross-map lag.

#How to interpret these plots...
All time series data were detrended and normalized before analysis.  I'm not sure detrending is the right approach with some of the data.  For example, the SSL and CSL data at Bonneville Dam has a string of zeros prior to their arrival in the 90's and 00's. This changes to a monotonic increase after detrending, so I removed the zeros. Other 'timeseries', including many for SSL, CSL and harbor seals are actually model generated. First differencing may be better in these cases.

Each cohort of salmon are only measured at one time point when they return to the river, so can we even use lags to test the directionality of causality? 

Causality was assessed using time-delay CCM on the rec4 and rec5 data with each of the putative causal variables. I had to leave out some of the shorter CCL and SSL time series as they weren't long enough.  All the plots show mean cross map skill for each possible pairwise interaction with standard deviation over 100 random libraries. I used the optimal embedding dimention of the TARGET variable in each analysis (according to Ye's EDM tutorial) and a tau=1 (time step of 1 year) for all analyses the data are not over sampled. The random libraries were bootstrap sampled with replacement and cross-map skill was computed using leave-one-out cross-validation over all the data. The closed circles indicate significant convergence with library size. 

Each plot shows two lines: the colored lines use lags of rec4 or rec5 to xmap the putative causal variables to evaluate their causal influence on rec4 or rec5. The black lines in each plot show the reverse - lags of the 'causal' variables are used to xmap the rec4 and rec5 data, so in this case we are looking for causal influence of salmon on the other variables. Negative rho values are meaningless and any bars that cross zero are not significantly different from zero. The x-axis is the time delay to prediction. 

Read the intro above, or see Ye et al. 2015 (Distinguishing time-delayed causal interactions using convergent cross mapping) for a detailed explanation, but basically if there is a true causal interaction the response variable is better at cross mapping past values of the putative causal variable. So, the optimal rho needs to peak at negative time points. However, the salmon timeseries are aligned with their brood year! So, the vertical line is not drawn at zero, but at the return year of the salmon. So rec4 has a vertical line drawn at tp=4 and rec5 have the line at tp=5. If there is bidirectional causality, we would see peaks on the left side of the lines for both the colored and black lines. However, if there isn't true bidirectional causality but a strong unidirectional interaction that is driving general synchrony, we would see a peak on the right side for the true driving variable cross-mapping the future of the response variable. If there simply is bidirectional causality, we would see negative peaks for both lines. A case like this might help pinpoint the truly strong driving variables in the system. 

A fewer of the plots show multiple peaks like with the orca data... I'm not sure how to think about this.  The length of the time delay may also have biological significance. For example, we would expect to see peaks for PDO at tp=2 because that is when they are most affected by PDO, supposedly. 



