---
title: "2_nonlinearity_test_CORE"
author: "Alison Iles"
date: "7/1/2020"
output:
  html_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(rEDM)
library(tseriesChaos)
library(multispatialCCM)
library(tidyr)
library(stringr)
library(zoo)

library(ggplot2)
library(ggforce)
library(gridExtra)
```
# This code runs several optimizations of parameters for nonlinear time series analysis

#1: Evaluate the optimal tau
The time-delay of the embedding, tau, is introduced to improve the observability of a time series (Judd and Mees 1998). The time lag is chosen to optimize the spread of the embedded time series without confusing the dynamics. The criteria for estimating a proper time lag is based on the following reasoning: if the time lag used to build the Takens' vectors is too small, the coordinates will be too highly temporally correlated and the embedding will tend to cluster around the diagonal in the phase space. If the time lag is chosen too large, the resulting coordinates may be almost uncorrelated and the resulting embedding will be very complicated. 

There are two principle methods for choosing the lags, the first zero of the autocorrelation function and the first local minimum of the time-delayed mutual information. However, it must be noted that the autocorrelation is a linear statistic, and thus it does not take into account nonlinear dynamical correlations. To take into account nonlinear correlations the average mutual information (AMI) is the most appropriate. 

Issues: the mutual information calculation cannot handle missing data points and multispatial data. It just treats the joined together stocks as one large data set. I will look for another algorithm that handels this better. For now, I'm calculating the 'optimal' tau (printed in the title of each subplot), but still considering all tau from 1 to 6 in the diagnostic plots for the embedding dimension and the test for non-linearity

#Find the optimal embedding dimension
The embedding dimension, E, is an estimate of the number of dimensions, or 'active variables', of the attractor underlying a time series. In practice, it is simply the OPTIMAL number of dimensions with which to unfold the attractor that maximizes the forecast skill (rho) of the embedding. Additional data may reveal additional dimensionality that was otherwise obscurred by process or observational noise.

Forecast skill (rho) between predicted and observed values determines the optimal model parameters.  The maximum E that can be tested depends on the length of the data as max E < sqrt(n).  We evaluated the optimal E for each tau from 1:6

#Test for nonlinearity in the SRKW time series 
Nonlinear state dependence dynamics frequently occur nature. Such nonlinearaties arise when variables have interdependent effects. For example, the strength of predation of orcas on chinook may depend on the relative abundance of pink salmon. Consequently, applying models that assume independent effects of causal variables can give the appearance of stochasticity and unpredictability, even when the underlying mechanics are deterministic. 

Thus, nonlinearity complicates the identification of causal drivers. Here we test whether the salmon population data exhibits non-linearity. Because the data is mulispatial, we use the SSR_pred_boot function to evaluate how forecast skill changes with the time to prediction. 

```{R}
# Final diagnostic analyses for rec4n, rec5n, recspn4n, and recspn5n for each MPG and ESU

rm(list=ls()) 
load("Data/Rdata/stock_data_no_lags.Rdata")  

vars <- c("stk", "year", "salm.rec4n", "salm.rec5n", "salm.recspn4n", "salm.recspn5n") 

#concatenate the stock time series together for each MPG and ESU
D_ESU <- as.matrix(rbind(stock_data$'Bear Valley Creek'[vars], NA,  
              stock_data$'Big Creek'[vars], NA,  
              stock_data$'Camas Creek'[vars], NA,  
              stock_data$'Catherine Creek'[vars], NA,  
              stock_data$'Chamberlain Creek'[vars], NA,  
              stock_data$'East Fork Salmon River'[vars], NA,  
              stock_data$'East Fork South Fork'[vars], NA,  
              stock_data$'Grande Ronde Upper Mainstem'[vars], NA,  
              stock_data$'Imnaha River'[vars], NA,  
              stock_data$'Lemhi River'[vars], NA,  
              stock_data$'Loon Creek'[vars], NA,  
              stock_data$'Marsh Creek'[vars], NA,  
              stock_data$'Middle Fork Salmon River above Indian Creek'[vars], NA,  
              stock_data$'Middle Fork Salmon River below Indian Creek'[vars], NA,  
              stock_data$'Minam River'[vars], NA,  
              stock_data$'North Fork Salmon River'[vars], NA,  
              stock_data$'Pahsimeroi River'[vars], NA,  
              stock_data$'Salmon River Lower Mainstem below Redfish Lake'[vars], NA,  
              stock_data$'Salmon River Upper Mainstem above Redfish Lake'[vars], NA,  
              stock_data$'Secesh River'[vars], NA,  
              stock_data$'South Fork Salmon River Mainstem'[vars], NA,  
              stock_data$'Sulphur Creek'[vars], NA,  
              stock_data$'Tucannon River'[vars], NA,  
              stock_data$'Valley Creek'[vars], NA,  
              stock_data$'Wallowa River, Hurricane Creek, Bear Creek, and Lostine Rivers'[vars], NA,  
              stock_data$'Wenaha River'[vars], NA,  
              stock_data$'Yankee Fork'[vars]))

D_MFS <- as.matrix(rbind(stock_data$'Bear Valley Creek'[vars], NA, 
           stock_data$'Big Creek'[vars], NA, 
           stock_data$'Camas Creek'[vars], NA, 
           stock_data$'Chamberlain Creek'[vars], NA, 
           stock_data$'Loon Creek'[vars], NA, 
           stock_data$'Marsh Creek'[vars], NA, 
           stock_data$'Middle Fork Salmon River above Indian Creek'[vars], NA, 
           stock_data$'Middle Fork Salmon River above Indian Creek'[vars], NA, 
           stock_data$'Sulphur Creek'[vars]))

D_IMN <- as.matrix(rbind(stock_data$'Catherine Creek'[vars], NA, 
                     stock_data$'Grande Ronde Upper Mainstem'[vars], NA, 
                     stock_data$'Imnaha River'[vars], NA, 
                     stock_data$'Minam River'[vars], NA, 
                     stock_data$'Wallowa River, Hurricane Creek, Bear Creek, and Lostine Rivers'[vars], NA, 
                     stock_data$'Wenaha River'[vars], NA))
  
D_UPS <- as.matrix(rbind(stock_data$'East Fork Salmon River'[vars], NA, 
                     stock_data$'Lemhi River'[vars], NA, 
                     stock_data$'North Fork Salmon River'[vars], NA, 
                     stock_data$'Pahsimeroi River'[vars], NA, 
                     stock_data$'Salmon River Lower Mainstem below Redfish Lake'[vars], NA, 
                     stock_data$'Salmon River Upper Mainstem above Redfish Lake'[vars], NA, 
                     stock_data$'Valley Creek'[vars], NA, 
                     stock_data$'Yankee Fork'[vars], NA))

# D_SFS <- as.matrix(rbind(stock_data$'East Fork South Fork'[vars], NA, 
#                      stock_data$'Secesh River'[vars], NA, 
#                      stock_data$'South Fork Salmon River Mainstem'[vars], NA))
  


#Function to remove to extra NAs, but leave one NA between each stock's data in an MPG
shape_stock_data <- function(data, min_stock_size = 20) {
         CC <- complete.cases(data[,3]) 
         lib <- matrix(NA, nrow = length(which(diff(CC)==1))+1, ncol = 2)
         lib[,1] <- c(1, (which(diff(CC)==1)+1))
         lib[,2] <- c((which(diff(CC)==-1)+1))
         #only include in the library the sections of data that are continuous for at least 20 time points. 
         minlib <- lib[,2]-lib[,1]
         lib <- lib[minlib>min_stock_size,] 
         x <- c(NA, NA, NA)
             for (r in 1:nrow(lib)){
                 xtmp <- data[lib[r,1]:lib[r,2],]
                 x <- rbind(x,xtmp)}
         data <- (x[2:(nrow(x)-1),])
         data <- as.data.frame(data)
         dataNA <- is.na(data[,3])
         data[dataNA,1:2] <- NA
         return(data)
}

D_ESU_r4n <- shape_stock_data(D_ESU[,c(1,2,3)], 10)
D_ESU_r5n <- shape_stock_data(D_ESU[,c(1,2,4)], 10)
D_ESU_rs4n <- shape_stock_data(D_ESU[,c(1,2,5)], 10)
D_ESU_rs5n <- shape_stock_data(D_ESU[,c(1,2,6)], 10)

D_MFS_r4n <- shape_stock_data(D_MFS[,c(1,2,3)], 10)
D_MFS_r5n <- shape_stock_data(D_MFS[,c(1,2,4)], 10)
D_MFS_rs4n <- shape_stock_data(D_MFS[,c(1,2,5)], 10)
D_MFS_rs5n <- shape_stock_data(D_MFS[,c(1,2,6)], 10)

D_IMN_r4n <- shape_stock_data(D_IMN[,c(1,2,3)], 10)
D_IMN_r5n <- shape_stock_data(D_IMN[,c(1,2,4)], 10)
D_IMN_rs4n <- shape_stock_data(D_IMN[,c(1,2,5)], 10)
D_IMN_rs5n <- shape_stock_data(D_IMN[,c(1,2,6)], 10)

D_UPS_r4n <- shape_stock_data(D_UPS[,c(1,2,3)], 10)
D_UPS_r5n <- shape_stock_data(D_UPS[,c(1,2,4)], 10)
D_UPS_rs4n <- shape_stock_data(D_UPS[,c(1,2,5)], 10)
D_UPS_rs5n <- shape_stock_data(D_UPS[,c(1,2,6)], 10)


## Run diagnostics for each MPG separately and the whole ESU
level <- c("ESU", "Middle Fork Salmon", "Imnaha", "Upper Salmon")
variables <- c("salm.rec4n.0", "salm.rec5n.0", "salm.recspn4n.0", "salm.recspn5n.0")
results <- matrix(nrow=16, ncol=4); colnames(results)<-c("level", "variable", "tau", "E") #Matrix for storing output
results[,2] <- variables

l <- 2; v <- 2
for(l in (2:4)){  #loop through each analysis level the ESU and the different MPGs

  subplot_list = list(); 
  for(v in 1:4){ #loop through the four different variables
      if(l==1 & v==1){D <- D_ESU_r4n; results[((l-1)*4 + 1):((l-1)*4 + 4),1] <- "ESU"}
      if(l==1 & v==2){D <- D_ESU_r5n}
      if(l==1 & v==3){D <- D_ESU_rs4n}
      if(l==1 & v==4){D <- D_ESU_rs5n}
      
      if(l==2 & v==1){D <- D_MFS_r4n; results[((l-1)*4 + 1):((l-1)*4 + 4),1] <- "MFS"}
      if(l==2 & v==2){D <- D_MFS_r5n}
      if(l==2 & v==3){D <- D_MFS_rs4n}
      if(l==2 & v==4){D <- D_MFS_rs5n}
      
      if(l==3 & v==1){D <- D_IMN_r4n; results[((l-1)*4 + 1):((l-1)*4 + 4),1] <- "IMN"}
      if(l==3 & v==2){D <- D_IMN_r5n}
      if(l==3 & v==3){D <- D_IMN_rs4n}
      if(l==3 & v==4){D <- D_IMN_rs5n}
      
      if(l==4 & v==1){D <- D_UPS_r4n; results[((l-1)*4 + 1):((l-1)*4 + 4),1] <- "UPS"}
      if(l==4 & v==2){D <- D_UPS_r5n}
      if(l==4 & v==3){D <- D_UPS_rs4n}
      if(l==4 & v==4){D <- D_UPS_rs5n}
          


#---------------------------- time-delay, tau ----------------------------
# the 'mutual' function that calculates the average mutual information needs a time series with no NA
Dt <- as.numeric(D[complete.cases(D[,3]),3])
AMI <- mutual(Dt, lag.max = 20, plot=FALSE)
minAMI <- min(which(diff(AMI)>=0))-1

  AMI <- as.data.frame(as.numeric(AMI)); 
  AMI <- cbind( c(0:(nrow(AMI)-1)), AMI); colnames(AMI) <- c("lag", "AMI")
  tau <- minAMI #choose the tau with the first local minimum AMI
  #tau <- AMI[AMI[,2]==min(AMI[1:5,2]),1] #choose the tau <=4 with the lowest AMI (sample size becomes too restrictive if we use bigger tau)
    results[((l-1)*4 + v),3] <- tau
  
  pAMI <- ggplot(AMI, aes(x=lag, y=AMI)) +
    geom_point() +
    theme_bw(base_size = 7) +
    theme(legend.position = "none") +
    labs(title = paste(variables[v], ", tau = ", tau, sep="")) 
 
subplot_list[[((v-1)*4 + 1)]] = pAMI

  
  
  
#---------------------------- lagged population plot ----------------------------

NAmat <- matrix(NA, ncol = 3, nrow=tau)
colnames(NAmat) <- colnames(D)
Lagblock <- cbind(rbind(NAmat, D), rbind(D, NAmat))
removerows <- Lagblock[,1]!=Lagblock[,4] | is.na(Lagblock[,1]) | is.na(Lagblock[,4])
Lagblock[removerows,] <- NA 
Lagblock <- Lagblock[complete.cases(Lagblock[,1]),]

Lagblock[,4:5] <- NULL
    colnames(Lagblock) <- c("stk", "year", "t0", "t-tau")
Lagblock$t0 <- as.numeric(Lagblock$t0)
Lagblock$`t-tau` <- as.numeric(Lagblock$`t-tau`)


 
pLag <- ggplot(Lagblock, aes(x=t0, y=`t-tau`, color=stk)) +
     geom_point(shape=1)+
     geom_hline(aes(yintercept=0), colour='#999999') +
     theme_bw(base_size = 10) + # big text
     theme(legend.position = "none") 
 
subplot_list[[((v-1)*4 + 2)]] = pLag




#---------------------------- Embedding dimension, E ----------------------------

A <- as.numeric(D[,3])

#set optimal E for each MPG and the ESU levels
maxE<-min(floor(sqrt(sum(complete.cases(A)))),15) #Maximum E to test based on the square root of the number of samples, also depends on tau

Emat<-matrix(nrow=maxE-1, ncol=8); colnames(Emat)<-c("E", "Tau1", "Tau2", "Tau3", "Tau4", "Tau5", "Tau6", "Tau7") #Matrix for storing output
#Loop over potential E values and save rho as an estimate of the ability of each process for its own dynamics


for(E in 2:maxE) {
  #Uses defaults of looking forward one prediction step (predstep)
  Emat[E-1,"E"] <- E
  Emat[E-1,"Tau1"] <- SSR_pred_boot(A=A, E=E, predstep=1, tau=1)$rho
  Emat[E-1,"Tau2"] <- SSR_pred_boot(A=A, E=E, predstep=1, tau=2)$rho
  Emat[E-1,"Tau3"] <- SSR_pred_boot(A=A, E=E, predstep=1, tau=3)$rho
  Emat[E-1,"Tau4"] <- SSR_pred_boot(A=A, E=E, predstep=1, tau=4)$rho
  Emat[E-1,"Tau5"] <- SSR_pred_boot(A=A, E=E, predstep=1, tau=5)$rho
  Emat[E-1,"Tau6"] <- SSR_pred_boot(A=A, E=E, predstep=1, tau=6)$rho
  Emat[E-1,"Tau7"] <- SSR_pred_boot(A=A, E=E, predstep=1, tau=7)$rho
}


#We defined E as the smallest dimension that came within 1% of the best predictive value observed across all dimensions with E ≤ sqrt(n), where
#n is time series length (Sugihara & May 1990; Sugihara et al. 2012; Ye et al. 2015; Karacoc et at 2020). 
maxrho=max(Emat[,"Tau1"], na.rm=TRUE)
    E1 <- min(Emat[(Emat[,"Tau1"]>=(maxrho-(maxrho*0.01))),"E"], na.rm=TRUE)
maxrho=max(Emat[,"Tau2"], na.rm=TRUE)
    E2 <- min(Emat[(Emat[,"Tau2"]>=(maxrho-(maxrho*0.01))),"E"], na.rm=TRUE)
maxrho=max(Emat[,"Tau3"], na.rm=TRUE)
    E3 <- min(Emat[(Emat[,"Tau3"]>=(maxrho-(maxrho*0.01))),"E"], na.rm=TRUE)
maxrho=max(Emat[,"Tau4"], na.rm=TRUE)
    E4 <- min(Emat[(Emat[,"Tau4"]>=(maxrho-(maxrho*0.01))),"E"], na.rm=TRUE)
maxrho=max(Emat[,"Tau5"], na.rm=TRUE)
    E5 <- min(Emat[(Emat[,"Tau5"]>=(maxrho-(maxrho*0.01))),"E"], na.rm=TRUE)
maxrho=max(Emat[,"Tau6"], na.rm=TRUE)
    E6 <- min(Emat[(Emat[,"Tau6"]>=(maxrho-(maxrho*0.01))),"E"], na.rm=TRUE)
maxrho=max(Emat[,"Tau7"], na.rm=TRUE)
    E7 <- min(Emat[(Emat[,"Tau7"]>=(maxrho-(maxrho*0.01))),"E"], na.rm=TRUE)

maxrho=max(Emat[,tau+1], na.rm=TRUE)    
results[((l-1)*4 + v),4] <- min(Emat[(Emat[,(tau+1)]>=(maxrho-(maxrho*0.01))),"E"], na.rm=TRUE)
            
Edf <- as.data.frame(Emat)
Edf <- pivot_longer(Edf, cols=c(2:8))
Edf$chosen <- 0
Edf$chosen [(Edf$E==E1 & Edf$name=="Tau1") | (Edf$E==E2 & Edf$name=="Tau2") | (Edf$E==E3 & Edf$name=="Tau3") | (Edf$E==E4 & Edf$name=="Tau4") | (Edf$E==E5 & Edf$name=="Tau5") | (Edf$E==E6 & Edf$name=="Tau6") | (Edf$E==E7 & Edf$name=="Tau7")] <- 1

#manual color codes for plot
pE <- ggplot(Edf, aes(x=E, y=value, color=name)) +
     geom_line() +
     geom_point(data=Edf[Edf$chosen==1,], aes(x=E, y=value, color=name), size=2, shape=1) +
     labs(x = "E", y = expression(rho)) +
     theme_bw() +
     theme(legend.position = "none")

subplot_list[[((v-1)*4 + 3)]] = pE 





#---------------------------- Time to prediction, TP ----------------------------
##Check data for nonlinear signal that is not dominated by noise
#Checks whether predictive ability of processes declines with increasing time distance
#Loop over the number of time steps into the future to make predictions from past observations. 
#Set 'matchSugi' to 0 to match results in Sugihara et al. publication, which removes all points within X(t-(E-1)):X(t+1) instead of just the target point

maxTP <- 50 #Maximum TP to test

TPdf <- matrix(nrow=maxTP, ncol=8); colnames(TPdf)<-c("TP", "Tau1", "Tau2", "Tau3", "Tau4", "Tau5", "Tau6", "Tau7") #Matrix for storing output
TPdf[,"TP"] <- c(1:maxTP) 
TPdf[,"Tau1"]<-SSR_check_signal(A=as.matrix(A), E=E1, tau=1, predsteplist=1:maxTP, matchSugi = 0)[[1]]$rho
TPdf[,"Tau2"]<-SSR_check_signal(A=as.matrix(A), E=E2, tau=2, predsteplist=1:maxTP, matchSugi = 0)[[1]]$rho
TPdf[,"Tau3"]<-SSR_check_signal(A=as.matrix(A), E=E3, tau=3, predsteplist=1:maxTP, matchSugi = 0)[[1]]$rho
TPdf[,"Tau4"]<-SSR_check_signal(A=as.matrix(A), E=E4, tau=4, predsteplist=1:maxTP, matchSugi = 0)[[1]]$rho
TPdf[,"Tau5"]<-SSR_check_signal(A=as.matrix(A), E=E5, tau=5, predsteplist=1:maxTP, matchSugi = 0)[[1]]$rho
TPdf[,"Tau6"]<-SSR_check_signal(A=as.matrix(A), E=E6, tau=6, predsteplist=1:maxTP, matchSugi = 0)[[1]]$rho
TPdf[,"Tau7"]<-SSR_check_signal(A=as.matrix(A), E=E7, tau=7, predsteplist=1:maxTP, matchSugi = 0)[[1]]$rho

TPdf <- as.data.frame(TPdf) 
TPdf <- pivot_longer(TPdf, cols=c(2:8))
pTP <- ggplot(TPdf, aes(x=TP, y=value, color=name) ) +
     geom_line() +
     labs(x = "time steps to prediction", y = expression(rho)) +
     theme_bw() +
     theme( legend.position = c(.99, .99),
              legend.justification = c("right", "top"),
              legend.box.just = "left",
              legend.margin=margin(0,0,0,0),
              legend.title = element_blank(),
              legend.text = element_text(size = 8),
              legend.background = element_blank(),
              legend.key.size = unit(0.3, "cm"))

subplot_list[[((v-1)*4 + 4)]] = pTP

}  #variables loop  


 n <- length(subplot_list)
 nCol <- 4 #floor(sqrt(n))
 pDIAGNOSTICS <- do.call("grid.arrange", c(subplot_list, ncol=nCol, top=level[l]))
ggsave(filename = paste("Output/Figures/2_Nonlinearity/",level[l],"_diagnostics.pdf", sep = ""), plot = pDIAGNOSTICS, width = 9, height = 9, units = "in")  # saves the last plot

} 

saveRDS(results, file = "Output/Rdata/2_Nonlinearity/Diagnostic_results.RDS")

```




# Run diagnostics for each stock separately



```{R}

rm(list=ls()) 
load("Data/Rdata/stock_data_no_lags.Rdata")  

results<-matrix(nrow=27, ncol=10); colnames(results)<-c("stock", "MPG",  "n4", "tau4", "E4", "rho4", "n5", "tau5", "E5", "rho5") #Storing output

sp <- 0
  rs4_subplot_list = list(); 
  rs5_subplot_list = list(); 
for(i in c(1:4, 6:12, 15, 18:27)){
  sp <- sp+1
  
  results[i,1] <- names(stock_data)[i]
  results[i,2] <- stock_data[[i]]$mpg[1]
  
  data <- stock_data[[i]]
  data <- data[c("stk", "year", "salm.recspn4n", "salm.recspn5n")]
  A4 <- na.trim(data[,c(1,2,3)], sides="both", is.na ="any")
  A5 <- na.trim(data[,c(1,2,4)], sides="both", is.na ="any") 
  
  results[i,3] <- nrow(A4)
  results[i,7] <- nrow(A5)
 
  #---------------------------- time-delay, tau ----------------------------
  # the 'mutual' function that calculates the average mutual information needs a time series with no NA
  AMI4 <- mutual(A4[,3], lag.max = 20, plot=FALSE)
  tau4 <- min(which(diff(AMI4)>=0))-1 #choose the tau with the first local minimum AMI
  AMI4 <- as.data.frame(as.numeric(AMI4));  
  AMI4 <- cbind( c(0:(nrow(AMI4)-1)), AMI4); colnames(AMI4) <- c("lag", "AMI")
  AMI4$chosen <- 0
  AMI4$chosen [ AMI4$lag==results[i,6]] <- 1
  pAMI4 <- ggplot(AMI4, aes(x=lag, y=AMI)) +
    geom_point() +
    geom_point(data=AMI4[AMI4$chosen==1,], aes(x=lag, y=AMI), size=3, shape=1) +
    theme_bw(base_size = 7) +
    theme(legend.position = "none") +
    labs(title = paste(results[i,1], ", tau = ", tau4, sep="")) 

  rs4_subplot_list[[((sp-1)*4 + 1)]] = pAMI4 
  
  
  AMI5 <- mutual(A5[,3], lag.max = 20, plot=FALSE)
  tau5 <- min(which(diff(AMI5)>=0))-1 #choose the tau with the first local minimum AMI
  AMI5 <- as.data.frame(as.numeric(AMI5));    
  AMI5 <- cbind( c(0:(nrow(AMI5)-1)), AMI5); colnames(AMI5) <- c("lag", "AMI")
  AMI5$chosen <- 0
  AMI5$chosen [ AMI5$lag==results[i,6]] <- 1
  pAMI5 <- ggplot(AMI5, aes(x=lag, y=AMI)) +
    geom_point() +
    geom_point(data=AMI5[AMI5$chosen==1,], aes(x=lag, y=AMI), size=3, shape=1) +
    theme_bw(base_size = 7) +
    theme(legend.position = "none") +
    labs(title = paste(results[i,1], ", tau = ", tau5, sep="")) 

  rs5_subplot_list[[((sp-1)*4 + 1)]] = pAMI5 


  
#---------------------------- lagged population plot ----------------------------

NAmat <- matrix(NA, ncol = 3, nrow=tau4)
colnames(NAmat) <- colnames(A4)
Lagblock <- cbind(rbind(NAmat, A4), rbind(A4, NAmat))
removerows <- Lagblock[,1]!=Lagblock[,4] | is.na(Lagblock[,1]) | is.na(Lagblock[,4])
Lagblock[removerows,] <- NA 
Lagblock <- Lagblock[complete.cases(Lagblock[,1]),]

Lagblock[,4:5] <- NULL
    colnames(Lagblock) <- c("stk", "year", "t0", "t-tau")
Lagblock$t0 <- as.numeric(Lagblock$t0)
Lagblock$`t-tau` <- as.numeric(Lagblock$`t-tau`)
 
pLag4 <- ggplot(Lagblock, aes(x=t0, y=`t-tau`, color=stk)) +
     geom_point(shape=1)+
     geom_hline(aes(yintercept=0), colour='#999999') +
     theme_bw(base_size = 10) + # big text
     theme(legend.position = "none") 
 
rs4_subplot_list[[((sp-1)*4 + 2)]] = pLag4

NAmat <- matrix(NA, ncol = 3, nrow=tau4)
colnames(NAmat) <- colnames(A5)
Lagblock <- cbind(rbind(NAmat, A5), rbind(A5, NAmat))
removerows <- Lagblock[,1]!=Lagblock[,4] | is.na(Lagblock[,1]) | is.na(Lagblock[,4])
Lagblock[removerows,] <- NA 
Lagblock <- Lagblock[complete.cases(Lagblock[,1]),]

Lagblock[,4:5] <- NULL
    colnames(Lagblock) <- c("stk", "year", "t0", "t-tau")
Lagblock$t0 <- as.numeric(Lagblock$t0)
Lagblock$`t-tau` <- as.numeric(Lagblock$`t-tau`)
 
pLag5 <- ggplot(Lagblock, aes(x=t0, y=`t-tau`, color=stk)) +
     geom_point(shape=1)+
     geom_hline(aes(yintercept=0), colour='#999999') +
     theme_bw(base_size = 10) + # big text
     theme(legend.position = "none") 

rs5_subplot_list[[((sp-1)*4 + 2)]] = pLag5




#---------------------------- Embedding dimension, E ----------------------------

#for recspn4:
tau <- min(5, tau4)
A <- as.numeric(A4[,3])
#set optimal E for each MPG and the ESU levels
maxE<-min(floor(sqrt(sum(complete.cases(A)))),15) #Maximum E to test based on the square root of the number of samples, also depends on tau

 Emat<-matrix(nrow=maxE-1, ncol=6); colnames(Emat)<-c("E", "Tau1", "Tau2", "Tau3", "Tau4", "Tau5") #Matrix for storing output
#Loop over potential E values and save rho as an estimate of the ability of each process for its own dynamics


for(E in 2:maxE) {
  #Uses defaults of looking forward one prediction step (predstep)
  Emat[E-1,"E"] <- E
  Emat[E-1,"Tau1"] <- SSR_pred_boot(A=A, E=E, predstep=1, tau=1)$rho
  Emat[E-1,"Tau2"] <- SSR_pred_boot(A=A, E=E, predstep=1, tau=2)$rho
  Emat[E-1,"Tau3"] <- SSR_pred_boot(A=A, E=E, predstep=1, tau=3)$rho
  Emat[E-1,"Tau4"] <- SSR_pred_boot(A=A, E=E, predstep=1, tau=4)$rho
  Emat[E-1,"Tau5"] <- SSR_pred_boot(A=A, E=E, predstep=1, tau=5)$rho
}

#We defined E as the smallest dimension that came within 1% of the best predictive value observed across all dimensions with E ≤ sqrt(n), where
#n is time series length (Sugihara & May 1990; Sugihara et al. 2012; Ye et al. 2015; Karacoc et at 2020). 
optE <- cbind(c(1:5), matrix(nrow=5, ncol=2)); colnames(optE)<-c("Tau", "optE", "rho")
for(t in 2:6) {
maxrho <- max(Emat[,t], na.rm=TRUE)
optE[t-1,3] <-  maxrho
    if(maxrho>0){
    optE[t-1, 2] <- min(Emat[(Emat[,t]>=(maxrho-(maxrho*0.01))),"E"], na.rm=TRUE)
    }
}
results[i,4:6] <- round(optE[optE[,3]==max(optE[,3], na.rm=TRUE),],2)
            
Edf <- as.data.frame(Emat)
Edf <- pivot_longer(Edf, cols=c(2:6))
Edf$chosen <- 0
Edf$chosen [Edf$E==results[i,5] & Edf$name==paste("Tau", results[i,4], sep="")] <- 1

#manual color codes for plot
pE4 <- ggplot(Edf, aes(x=E, y=value, color=name)) +
     geom_line() +
     geom_point(data=Edf[Edf$chosen==1,], aes(x=E, y=value, color=name), size=2, shape=1) +
     labs(x = "E", y = expression(rho)) +
     theme_bw() +
     theme(legend.position = "none")

rs4_subplot_list[[((sp-1)*4 + 3)]] = pE4


# Time to prediction, TP recspn4
##Check data for nonlinear signal that is not dominated by noise
#Checks whether predictive ability of processes declines with increasing time distance
#Loop over the number of time steps into the future to make predictions from past observations. 
#Set 'matchSugi' to 0 to match results in Sugihara et al. publication, which removes all points within X(t-(E-1)):X(t+1) instead of just the target point
maxTP <- 10 #Maximum TP to test

TPdf <- matrix(nrow=maxTP, ncol=6); colnames(TPdf)<-c("TP", "Tau1", "Tau2", "Tau3", "Tau4", "Tau5") #Matrix for storing output
TPdf[,"TP"] <- c(1:maxTP) 
for(t in 2:6) {
  if(is.na(optE[t-1,2])==FALSE ){
TPdf[,t]<-SSR_check_signal(A=as.matrix(A), E=optE[t-1,2], tau=t-1, predsteplist=1:maxTP, matchSugi = 0)[[1]]$rho}
}

TPdf <- as.data.frame(TPdf) 
TPdf <- pivot_longer(TPdf, cols=c(2:6))
pTP4 <- ggplot(TPdf, aes(x=TP, y=value, color=name) ) +
     geom_line() +
     labs(x = "time steps to prediction", y = expression(rho)) +
     theme_bw() +
     theme( legend.position = c(.99, .99),
              legend.justification = c("right", "top"),
              legend.box.just = "left",
              legend.margin=margin(0,0,0,0),
              legend.title = element_blank(),
              legend.text = element_text(size = 8),
              legend.background = element_blank(),
              legend.key.size = unit(0.3, "cm"))

rs4_subplot_list[[((sp-1)*4 + 4)]] = pTP4



#------------------------------------------------------------------------------------
#for recspn5:
tau <- min(5, tau5)
A <- as.numeric(A5[,3])

#set optimal E for each MPG and the ESU levels
maxE<-min(floor(sqrt(sum(complete.cases(A)))),15) #Maximum E to test based on the square root of the number of samples, also depends on tau

 Emat<-matrix(nrow=maxE-1, ncol=6); colnames(Emat)<-c("E", "Tau1", "Tau2", "Tau3", "Tau4", "Tau5") #Matrix for storing output
#Loop over potential E values and save rho as an estimate of the ability of each process for its own dynamics


for(E in 2:maxE) {
  #Uses defaults of looking forward one prediction step (predstep)
  Emat[E-1,"E"] <- E
  Emat[E-1,"Tau1"] <- SSR_pred_boot(A=A, E=E, predstep=1, tau=1)$rho
  Emat[E-1,"Tau2"] <- SSR_pred_boot(A=A, E=E, predstep=1, tau=2)$rho
  Emat[E-1,"Tau3"] <- SSR_pred_boot(A=A, E=E, predstep=1, tau=3)$rho
  Emat[E-1,"Tau4"] <- SSR_pred_boot(A=A, E=E, predstep=1, tau=4)$rho
  Emat[E-1,"Tau5"] <- SSR_pred_boot(A=A, E=E, predstep=1, tau=5)$rho
}

#We defined E as the smallest dimension that came within 1% of the best predictive value observed across all dimensions with E ≤ sqrt(n), where
#n is time series length (Sugihara & May 1990; Sugihara et al. 2012; Ye et al. 2015; Karacoc et at 2020). 
optE <- cbind(c(1:5), matrix(nrow=5, ncol=2)); colnames(optE)<-c("Tau", "optE", "rho")
for(t in 2:6) {
maxrho <- max(Emat[,t], na.rm=TRUE)
optE[t-1,3] <-  maxrho
    if(maxrho>0){
    optE[t-1, 2] <- min(Emat[(Emat[,t]>=(maxrho-(maxrho*0.01))),"E"], na.rm=TRUE)
    }
}
results[i,8:10] <- round(optE[optE[,3]==max(optE[,3], na.rm=TRUE),],2)
            
Edf <- as.data.frame(Emat)
Edf <- pivot_longer(Edf, cols=c(2:6))
Edf$chosen <- 0
Edf$chosen [Edf$E==results[i,9] & Edf$name==paste("Tau", results[i,8], sep="")] <- 1

#manual color codes for plot
pE5 <- ggplot(Edf, aes(x=E, y=value, color=name)) +
     geom_line() +
     geom_point(data=Edf[Edf$chosen==1,], aes(x=E, y=value, color=name), size=2, shape=1) +
     labs(x = "E", y = expression(rho)) +
     theme_bw() +
     theme(legend.position = "none")

rs5_subplot_list[[((sp-1)*4 + 3)]] = pE5 


#Time to prediction, TP recspn 5
##Check data for nonlinear signal that is not dominated by noise
#Checks whether predictive ability of processes declines with increasing time distance
#Loop over the number of time steps into the future to make predictions from past observations. 
#Set 'matchSugi' to 0 to match results in Sugihara et al. publication, which removes all points within X(t-(E-1)):X(t+1) instead of just the target point

maxTP <- 10 #Maximum TP to test

TPdf <- matrix(nrow=maxTP, ncol=6); colnames(TPdf)<-c("TP", "Tau1", "Tau2", "Tau3", "Tau4", "Tau5") #Matrix for storing output
TPdf[,"TP"] <- c(1:maxTP) 
for(t in 2:6) {
  if(is.na(optE[t-1,2])==FALSE ){
        #ERROR HANDLING
        possibleError <- tryCatch(
            SSR_check_signal(A=as.matrix(A), E=optE[t-1,2], tau=t-1, predsteplist=1:maxTP, matchSugi = 0),
            error=function(e) e)
        if(inherits(possibleError, "error")) next
  
        #REAL WORK IF NO ERROR 
        TPdf[,t]<-SSR_check_signal(A=as.matrix(A), E=optE[t-1,2], tau=t-1, predsteplist=1:maxTP, matchSugi = 0)[[1]]$rho}
  }

TPdf <- as.data.frame(TPdf) 
TPdf <- pivot_longer(TPdf, cols=c(2:6))
pTP5 <- ggplot(TPdf, aes(x=TP, y=value, color=name) ) +
     geom_line() +
     labs(x = "time steps to prediction", y = expression(rho)) +
     theme_bw() +
     theme( legend.position = c(.99, .99),
              legend.justification = c("right", "top"),
              legend.box.just = "left",
              legend.margin=margin(0,0,0,0),
              legend.title = element_blank(),
              legend.text = element_text(size = 8),
              legend.background = element_blank(),
              legend.key.size = unit(0.3, "cm"))

rs5_subplot_list[[((sp-1)*4 + 4)]] = pTP5

}


p4 <- marrangeGrob(rs4_subplot_list, nrow=4, ncol=4)
ggsave(filename = paste("Output/Figures/2_Nonlinearity/recspn4_stock_level_diagnostic_plots.pdf", sep = ""), plot = p4, width = 9, height = 9, units = "in")

p5 <- marrangeGrob(rs5_subplot_list, nrow=4, ncol=4)
ggsave(filename = paste("Output/Figures/2_Nonlinearity/recspn5_stock_level_diagnostic_plots.pdf", sep = ""), plot = p5, width = 9, height = 9, units = "in")

```

```{R}
# Run diagnostics for each MPG and ESU at Tau from 1:5

load("Data/Rdata/stock_data_no_lags.Rdata")  


#choose the library variable to analyze
vars <- c("stk", "year", "salm.recspn4n", "salm.recspn5n") 

#concatenate the stock time series together for each MPG and ESU
D_MFS <- as.matrix(rbind(stock_data$'Bear Valley Creek'[vars], c(NA, NA), 
           stock_data$'Big Creek'[vars], c(NA, NA), 
           stock_data$'Camas Creek'[vars], c(NA, NA), 
           stock_data$'Chamberlain Creek'[vars], c(NA, NA), 
           stock_data$'Loon Creek'[vars], c(NA, NA), 
           stock_data$'Marsh Creek'[vars], c(NA, NA), 
           stock_data$'Middle Fork Salmon River above Indian Creek'[vars], c(NA, NA), 
           stock_data$'Middle Fork Salmon River above Indian Creek'[vars], c(NA, NA), 
           stock_data$'Sulphur Creek'[vars]))

D_IMN <- as.matrix(rbind(stock_data$'Catherine Creek'[vars], c(NA, NA), 
                     stock_data$'Grande Ronde Upper Mainstem'[vars], c(NA, NA), 
                     stock_data$'Imnaha River'[vars], c(NA, NA), 
                     stock_data$'Lostine Creek'[vars], c(NA, NA), 
                     stock_data$'Minam River'[vars], c(NA, NA), 
                     stock_data$'Wallowa River, Hurricane Creek, Bear Creek, and Lostine Rivers'[vars], c(NA, NA), 
                     stock_data$'Wenaha River'[vars], c(NA, NA)))
  
D_UPS <- as.matrix(rbind(stock_data$'East Fork Salmon River'[vars], c(NA, NA), 
                     stock_data$'Lemhi River'[vars], c(NA, NA), 
                     stock_data$'North Fork Salmon River'[vars], c(NA, NA), 
                     stock_data$'Pahsimeroi River'[vars], c(NA, NA), 
                     stock_data$'Salmon River Lower Mainstem below Redfish Lake'[vars], c(NA, NA), 
                     stock_data$'Salmon River Upper Mainstem above Redfish Lake'[vars], c(NA, NA), 
                     stock_data$'Valley Creek'[vars], c(NA, NA), 
                     stock_data$'Yankee Fork'[vars], c(NA, NA)))

D_SFS <- as.matrix(rbind(stock_data$'East Fork South Fork'[vars], c(NA, NA), 
                     stock_data$'Secesh River'[vars], c(NA, NA), 
                     stock_data$'South Fork Salmon River Mainstem'[vars], c(NA, NA)))
  
D_ESU <- as.matrix(rbind(stock_data$'Bear Valley Creek'[vars], c(NA, NA),  
              stock_data$'Big Creek'[vars], c(NA, NA),  
              stock_data$'Camas Creek'[vars], c(NA, NA),  
              stock_data$'Catherine Creek'[vars], c(NA, NA),  
              stock_data$'Chamberlain Creek'[vars], c(NA, NA),  
              stock_data$'East Fork Salmon River'[vars], c(NA, NA),  
              stock_data$'East Fork South Fork'[vars], c(NA, NA),  
              stock_data$'Grande Ronde Upper Mainstem'[vars], c(NA, NA),  
              stock_data$'Imnaha River'[vars], c(NA, NA),  
              stock_data$'Lemhi River'[vars], c(NA, NA),  
              stock_data$'Loon Creek'[vars], c(NA, NA),  
              stock_data$'Lostine Creek'[vars], c(NA, NA),  
              stock_data$'Marsh Creek'[vars], c(NA, NA),  
              stock_data$'Middle Fork Salmon River above Indian Creek'[vars], c(NA, NA),  
              stock_data$'Middle Fork Salmon River below Indian Creek'[vars], c(NA, NA),  
              stock_data$'Minam River'[vars], c(NA, NA),  
              stock_data$'North Fork Salmon River'[vars], c(NA, NA),  
              stock_data$'Pahsimeroi River'[vars], c(NA, NA),  
              stock_data$'Salmon River Lower Mainstem below Redfish Lake'[vars], c(NA, NA),  
              stock_data$'Salmon River Upper Mainstem above Redfish Lake'[vars], c(NA, NA),  
              stock_data$'Secesh River'[vars], c(NA, NA),  
              stock_data$'South Fork Salmon River Mainstem'[vars], c(NA, NA),  
              stock_data$'Sulphur Creek'[vars], c(NA, NA),  
              stock_data$'Tucannon River'[vars], c(NA, NA),  
              stock_data$'Valley Creek'[vars], c(NA, NA),  
              stock_data$'Wallowa River, Hurricane Creek, Bear Creek, and Lostine Rivers'[vars], c(NA, NA),  
              stock_data$'Wenaha River'[vars], c(NA, NA),  
              stock_data$'Yankee Fork'[vars]))


#Function to remove to extra NAs, but leave one NA between each stock's data in an MPG
shape_stock_data <- function(data, min_stock_size = 20) {
         CC <- complete.cases(data[,3]) 
         lib <- matrix(NA, nrow = length(which(diff(CC)==1))+1, ncol = 2)
         lib[,1] <- c(1, (which(diff(CC)==1)+1))
         lib[,2] <- c((which(diff(CC)==-1)+1))
         #only include in the library the sections of data that are continuous for at least 20 time points. 
         minlib <- lib[,2]-lib[,1]
         lib <- lib[minlib>min_stock_size,] 
         x <- c(NA, NA, NA)
             for (r in 1:nrow(lib)){
                 xtmp <- data[lib[r,1]:lib[r,2],]
                 x <- rbind(x,xtmp)}
         data <- (x[2:(nrow(x)-1),])
         data <- as.data.frame(data)
         dataNA <- is.na(data[,3])
         data[dataNA,1:2] <- NA
         return(data)
}

D_MFS4 <- shape_stock_data(D_MFS[,1:3], 20)
D_MFS5 <- shape_stock_data(D_MFS[,c(1,2,4)], 20)

D_IMN4 <- shape_stock_data(D_IMN[,1:3], 20) 
D_IMN5 <- shape_stock_data(D_IMN[,c(1,2,4)], 20)

D_UPS4 <- shape_stock_data(D_UPS[,1:3], 20) 
D_UPS5 <- shape_stock_data(D_UPS[,c(1,2,4)], 20) 

D_SFS4 <- shape_stock_data(D_SFS[,1:3], 20) 
D_SFS5 <- shape_stock_data(D_SFS[,c(1,2,4)], 20) 

D_ESU4 <- shape_stock_data(D_ESU[,1:3], 20) 
D_ESU5 <- shape_stock_data(D_ESU[,c(1,2,4)], 20)  
```

```{R}
## Run diagnostics for each MPG separately and the whole ESU
level <- c("ESU", "Middle Fork Salmon", "Imnaha", "Upper Salmon", "South Fork Salmon")
levelname <- c("ESU", "MiddleForkSalmon", "Imnaha", "UpperSalmon", "SouthForkSalmon") 

for(l in (1:5)){  #loop through the ESU and the different MPGs
  if(l==1){Dtemp4 <- D_ESU4; Dtemp5 <- D_ESU5}
  if(l==2){Dtemp4 <- D_MFS4; Dtemp5 <- D_MFS5}
  if(l==3){Dtemp4 <- D_IMN4; Dtemp5 <- D_IMN5}
  if(l==4){Dtemp4 <- D_UPS4; Dtemp5 <- D_UPS5}
  if(l==5){Dtemp4 <- D_SFS4; Dtemp5 <- D_SFS5}
 
subplot_list = list(); sp <- 1
for(tau in -1:-6){  #loop through the different values of tau
  
#---------------------------- lagged population plot ----------------------------

NAmat <- matrix(NA, ncol = ncol(Dtemp4), nrow=-tau)
colnames(NAmat) <- colnames(Dtemp4)
Lagblock <- cbind(rbind(NAmat, Dtemp4), rbind(Dtemp4, NAmat))
Lagblock[,4:5] <- NULL
    colnames(Lagblock) <- c("stk", "year", "t0", "t-tau")
Lagblock$t0 <- as.numeric(Lagblock$t0)
Lagblock$`t-tau` <- as.numeric(Lagblock$`t-tau`)
 
pL4 <- ggplot(Lagblock, aes(x=t0, y=`t-tau`, color=stk)) +
     geom_point(shape=1)+
     geom_hline(aes(yintercept=0), colour='#999999') +
     theme_bw(base_size = 10) + # big text
     theme(legend.position = "none") +
     labs(title = paste("recspn4n, tau = ", tau, sep="")) 
 
NAmat <- matrix(NA, ncol = ncol(Dtemp5), nrow=-tau)
colnames(NAmat) <- colnames(Dtemp5)
Lagblock <- cbind(rbind(NAmat, Dtemp5), rbind(Dtemp5, NAmat))
Lagblock[,4:5] <- NULL
    colnames(Lagblock) <- c("stk", "year", "t0", "t-tau")
Lagblock$t0 <- as.numeric(Lagblock$t0)
Lagblock$`t-tau` <- as.numeric(Lagblock$`t-tau`)
 
pL5 <- ggplot(Lagblock, aes(x=t0, y=`t-tau`, color=stk)) +
     geom_point(shape=1)+
     geom_hline(aes(yintercept=0), colour='#999999') +
     theme_bw(base_size = 10) + # big text
     theme(legend.position = "none") +
     labs(title = paste("recspn5n, tau = ", tau, sep="")) 

#---------------------------- Embedding dimension, E ----------------------------


#set optimal E for each MPG and the ESU levels
maxE<-20 #Maximum E to test based on the square root of the number of samples

Emat<-matrix(nrow=maxE-1, ncol=3); colnames(Emat)<-c("E", "rec4n rho", "rec5n rho") #Matrix for storing output
#Loop over potential E values and save rho as an estimate of the ability of each process for its own dynamics

A4 <- as.numeric(Dtemp4$salm.recspn4n)
A5 <- as.numeric(Dtemp5$salm.recspn5n)



for(E in 2:maxE) {
  #Uses defaults of looking forward one prediction step (predstep)
  Emat[E-1,"E"] <- E
  Emat[E-1,"rec4n"] <- SSR_pred_boot(A=A4, E=E, predstep=1, tau=-tau)$rho
  Emat[E-1,"rec5n"] <- SSR_pred_boot(A=A5, E=E, predstep=1, tau=-tau)$rho
}

#choose smallest E that within 0.05 of the maximum rho, so as not to overfit the data
E4 <- min(Emat[(Emat[,"rec4n"]==(max(Emat[,"rec4n"], na.rm=TRUE))),"E"], na.rm=TRUE)
E5 <- min(Emat[(Emat[,"rec5n"]==(max(Emat[,"rec5n"], na.rm=TRUE))),"E"], na.rm=TRUE)


Edf <- as.data.frame(Emat)
Edf <- pivot_longer(Edf, cols=c(2:3))
Edf$chosen <- 0
Edf$chosen [(Edf$E==E4 & Edf$name=="rec4n") | (Edf$E==E5 & Edf$name=="rec5n")] <- 1

#manual color codes for plot
mcc <- cbind(c('#f58231', '#469990'), c("rec4n", "rec5n"), c("rec4n", "rec5n")); colnames(mcc) <- c("values", "breaks", "labels")
pE <- ggplot(Edf, aes(x=E, y=value, color=name)) +
     geom_line() +
     geom_point(data=Edf[Edf$chosen==1,], aes(x=E, y=value, color=name), size=2, shape=1) +
     scale_color_manual(values = mcc[,1], name="", breaks = mcc[,2], labels = mcc[,3]) +
     labs(x = "E", y = expression(rho)) +
     theme_bw()+
     theme( legend.position = c(.01, .01),
              legend.justification = c("left", "bottom"),
              legend.box.just = "left",
              legend.margin=margin(0,0,0,0),
              legend.title = element_blank(),
              legend.text = element_text(size = 6),
              legend.background = element_blank(),
              legend.key.size = unit(0.3, "cm"))




#---------------------------- Time to prediction, TP ----------------------------
##Check data for nonlinear signal that is not dominated by noise
#Checks whether predictive ability of processes declines with increasing time distance
#Loop over the number of time steps into the future to make predictions from past observations. 
#Set 'matchSugi' to 0 to match results in Sugihara et al. publication, which removes all points within X(t-(E-1)):X(t+1) instead of just the target point

maxTP <- 50 #Maximum TP to test

TPmat <- matrix(nrow=maxTP, ncol=3); colnames(TPmat)<-c("TP", "rec4n", "rec5n") #Matrix for storing output
TPmat[,"TP"] <- c(1:maxTP) 
TPmat[,"rec4n"]<-SSR_check_signal(A=as.matrix(A4), E=E4, tau=-tau, predsteplist=1:maxTP, matchSugi = 0)[[1]]$rho
TPmat[,"rec5n"]<-SSR_check_signal(A=as.matrix(A5), E=E5, tau=-tau, predsteplist=1:maxTP, matchSugi = 0)[[1]]$rho


TPdf <- as.data.frame(TPmat) 
TPdf <- pivot_longer(TPdf, cols=c(2:3))
pTP <- ggplot(TPdf, aes(x=TP, y=value, color=name) ) +
     geom_line() +
     scale_color_manual(values = mcc[,1], name="", breaks = mcc[,2], labels = mcc[,3]) +
     labs(x = "time steps to prediction", y = expression(rho)) +
     theme_bw() +
     theme(legend.position = "none")





subplot_list[[(1+((-tau-1)*4))]] = pL4
subplot_list[[(2+((-tau-1)*4))]] = pL5
subplot_list[[(3+((-tau-1)*4))]] = pE
subplot_list[[(4+((-tau-1)*4))]] = pTP


}  #tau loop

p4 <- marrangeGrob(rs4_subplot_list, nrow=4, ncol=4)
ggsave(filename = paste("Output/Figures/2_Nonlinearity/recspn4_stock_level_diagnostic_plots.pdf", sep = ""), plot = p4, width = 9, height = 9, units = "in")

p5 <- marrangeGrob(rs5_subplot_list, nrow=4, ncol=4)
ggsave(filename = paste("Output/Figures/2_Nonlinearity/recspn5_stock_level_diagnostic_plots.pdf", sep = ""), plot = p5, width = 9, height = 9, units = "in")

}  #ESU and MPG levels loop
 
```









# The following is the older version of our diagnostics that uses the rEDM package that is not optimized for multispatial data. 
```{R}
test_nonlin <- function(ts, method = "ebisuzaki", num_surr = 200, 
                              T_period = 1, E = 3, ...)
{
    compute_test_stat <- function(ts, ...)
    {
        results <- s_map(ts, stats_only = TRUE, silent = TRUE, ...)
        delta_rho <- max(results$rho) - results$rho[results$theta == 0]
        delta_mae <- results$mae[results$theta == 0] - min(results$mae)
        return(c(delta_rho = delta_rho, delta_mae = delta_mae))
    }
    
    actual_stats <- compute_test_stat(ts, ...)
    delta_rho <- actual_stats["delta_rho"]
    delta_mae <- actual_stats["delta_mae"]
    names(delta_rho) <- NULL
    names(delta_mae) <- NULL
    surrogate_data <- make_surrogate_ebisuzaki(ts) #generate surrogate data 
    null_stats <- data.frame(t(apply(surrogate_data, 2, compute_test_stat, ...)))
    
    return(data.frame(
        delta_rho = delta_rho, 
        delta_mae = delta_mae, 
        num_surr = num_surr, 
        E = E, 
        delta_rho_p_value = (sum(null_stats$delta_rho > delta_rho) + 1) / 
            (num_surr + 1), 
        delta_mae_p_value = (sum(null_stats$delta_mae > delta_mae) + 1) / 
            (num_surr + 1)))
}

      #set up output table for the best E, theta, tau and the corresponding EDM rho using simplex and smap methods, the change in rho with theta and its significance test against surrogate data

         # best E and theta based on optimizing rho
         best_E_theta_rho <- matrix(NA, nrow = length(library_vars), ncol = 7) 
         colnames(best_E_theta_rho) <- c("library", "E", "theta", "simplex_rho", "smap_rho", "delta_rho", "delta_rho_p")
         best_E_theta_rho[,1] <- t(library_vars)
         
         # best E and theta based on optimizing mae
         best_E_theta_mae <- matrix(NA, nrow = length(library_vars), ncol = 7) 
         colnames(best_E_theta_mae) <- c("library", "E", "theta", "simplex_rho", "smap_rho", "delta_mae", "delta_mae_p")
         best_E_theta_mae[,1] <- t(library_vars)
 
#plot the result to evaluate E, Tp, and       
layout(matrix(c(1,2,3,4,5,6), 3, 2, byrow = TRUE))

    for(j in 1:length(library_vars)){ #for each library variable
         
     D <- as.matrix(cbind(Dtemp$year, Dtemp[library_vars[j]]))
            rownames(D) <- NULL #remove rownames to supress error in CCM: NAs introduced by coersion
            D <- D[complete.cases(D), ]#remove rows with NA
            colnames(D) <- c("year", library_vars[j])
         
      #List in lib the begin and end points of each stock and the break points within stocks
         lib <- matrix(NA, nrow = length(which(diff(D[,1])!=1))+1, ncol = 2)
         lib[,1] <- c(1, which(diff(D[,1])!=1)+1)
         lib[,2] <- c(which(diff(D[,1])!=1), nrow(D))
      #only include in the library the sections of data that are continuous for at least 20 time points. 
         minlib <- lib[,2]-lib[,1]
         lib <- lib[minlib>20,] 
         x <- D[lib[1,1]:lib[1,2],] #narrow the data block to these library sections
             for (r in 2:nrow(lib)){
                 xtmp <- D[lib[r,1]:lib[r,2],]
                 x <- rbind(x,xtmp)}
         D <- x
      # need to recreate the library list
         lib <- matrix(NA, nrow = length(which(diff(D[,1])!=1))+1, ncol = 2)
              lib[,1] <- c(1, which(diff(D[,1])!=1)+1)
              lib[,2] <- c(which(diff(D[,1])!=1), nrow(D))
              rm(x,minlib, xtmp, r)
        
         
      
    #Determine best embedding dimension
      Simp_out <- simplex(D, lib=lib, E = 1:12, tau=1, silent = TRUE)
            best_E_theta_rho[j,2] <- min(Simp_out$E[Simp_out$rho>max(Simp_out$rho)-(sd(Simp_out$rho)/2)]) #choose the smallest E whose rho is within 0.5 stdv of the maximum rho
            best_E_theta_rho[j,4] <- round(Simp_out$rho[Simp_out$E==best_E_theta_rho[j,2]], digits = 2)
            
            best_E_theta_mae[j,2] <- min(Simp_out$E[Simp_out$mae<min(Simp_out$mae)+(sd(Simp_out$mae)/2)])
            best_E_theta_mae[j,4] <- round(Simp_out$rho[Simp_out$E==best_E_theta_mae[j,2]], digits = 2)
            
       plot(Simp_out$E, Simp_out$rho, type="l", xlab = "Embedding dimension (E)", ylab="Forecast skill (rho)", main=paste(levelname[l], ": ", library_vars[j], ", E = ", best_E_theta_rho[j,2]))
       plot(Simp_out$E, Simp_out$mae, type="l", xlab = "Embedding dimension (E)", ylab="Mean absolute error (mae)", main=paste(levelname[l], ": ", library_vars[j], ", E = ", best_E_theta_mae[j,2]))
       
     #Modify the time to prediction to test for deterministic chaos vs stochasticity
      Tp_out_rho <- simplex(D, lib=lib, E = as.numeric(best_E_theta_rho[j,2]), tau=1, tp = 1:12, silent = TRUE)
      Tp_out_mae <- simplex(D, lib=lib, E = as.numeric(best_E_theta_mae[j,2]), tau=1, tp = 1:12, silent = TRUE)

      plot(Tp_out_rho$tp, Tp_out_rho$rho, type="l", xlab = "Time to prediction (Tp)", ylab="Forecast skill (rho)", main=paste(levelname[l], ": ", "Time to prediction"))
      plot(Tp_out_mae$tp, Tp_out_rho$mae, type="l", xlab = "Time to prediction (Tp)", ylab="Mean absolute error (mae)", main=paste(levelname[l], ": ", "Time to prediction"))
      
     #Determine the optimal theta and test that it is significantly different from zero  
      #*****use overall best E of 5 for all********
      Smap_out <-s_map(D, lib=lib, E=5, tau=1, silent = TRUE) #E = as.numeric(best_E_theta_rho[j,2])
            best_E_theta_rho[j,3] <-  min(Smap_out$theta[Smap_out$rho==max(Smap_out$rho)]) #choose the theta that maximizes rho
            best_E_theta_rho[j,5] <- round(max(Smap_out$rho), digits = 2)

            test_nonlin_out <- test_nonlin(ts = D, E = as.numeric(best_E_theta_rho[j,2]), lib=lib)
            best_E_theta_rho[j,6] <- round(test_nonlin_out$delta_rho, digits = 2)
            best_E_theta_rho[j,7] <- round(test_nonlin_out$delta_rho_p_value, digits = 2)
       
      plot(Smap_out$theta, Smap_out$rho, type="l", xlab = "theta ()", ylab="Forecast skill (rho)", main=paste(levelname[l], ": ", library_vars[j], " theta = ", best_E_theta_rho[j,3]))
      
      Smap_out <-s_map(D, lib=lib, E=5, tau=1, silent = TRUE)      #E = as.numeric(best_E_theta_mae[j,2])
            best_E_theta_mae[j,3] <-  min(Smap_out$theta[Smap_out$mae==min(Smap_out$mae)]) #choose the theta that minimizes mae
            best_E_theta_mae[j,5] <- round(Smap_out$rho[Smap_out$mae==min(Smap_out$mae, na.rm = TRUE)], digits = 2)

            test_nonlin_out <- test_nonlin(ts = D, E = as.numeric(best_E_theta_mae[j,2]), lib=lib)
            best_E_theta_mae[j,6] <- round(test_nonlin_out$delta_mae, digits = 2)
            best_E_theta_mae[j,7] <- round(test_nonlin_out$delta_mae_p_value, digits = 2)
  
      plot(Smap_out$theta, Smap_out$mae, type="l", xlab = "theta ()", ylab="Mean absolute error (mae)", main=paste(levelname[l], ": ", library_vars[j], " theta = ", best_E_theta_mae[j,3]))
            
    }
    
    print(best_E_theta_rho)
    saveRDS(best_E_theta_rho, file = paste("Output/Rdata/2_Nonlinearity/best_E_theta_rho_recspn_", levelname[l], ".RDS", sep = ""), compress = FALSE)
    print(best_E_theta_mae)
    saveRDS(best_E_theta_mae, file = paste("Output/Rdata/2_Nonlinearity/best_E_theta_mae_recspn_", levelname[l], ".RDS", sep = ""), compress = FALSE)

layout(matrix(c(1,1), 1, 1, byrow = TRUE))

}

dev.off()
```
Results for embedding dimension, time to prediction, nonlinearity and predictability